{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_WFbYH_YYOt"
      },
      "source": [
        "### üß™ Smart AI - RAG with Role-Based Prompting\n",
        "\n",
        "This extand version of the Smart Document Helper implements **Role Prompting / Persona Simulation** to improve response quality and user experience.\n",
        "\n",
        "**Key featurs:**\n",
        "\n",
        "- üé≠ **Multiple Personas**: Choose from Teacher, Expert Reviewer, Legal Advisor, Technical Writer, or Friendly Assistant\n",
        "- üéØ **Role-Specific Prompts**: Each persona has tailored prompt templates for different tasks\n",
        "- üìä **Response Quality**: Responses are contextualized based on the selected role\n",
        "- üîÑ **Dynamic Interaction**: Users can switch between roles for different perspectives\n",
        "\n",
        "**Available Roles:**\n",
        "- üë©‚Äçüè´ **Teacher**: Explains concepts clearly with examples, pedagogical approach\n",
        "- üîç **Expert Reviewer**: Critical analysis, detailed evaluation, professional tone\n",
        "- ‚öñÔ∏è **Legal Advisor**: Formal language, cautious interpretation, disclaimer-aware\n",
        "- üíª **Technical Writer**: Structured, precise, documentation-style responses\n",
        "- üòä **Friendly Assistant**: Conversational, helpful, approachable tone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--f_ZlmLYw5O"
      },
      "source": [
        "### üîß Part 0: Setup and Installations\n",
        "\n",
        "Install the required libraries for our RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6G7oY16S2e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: faiss-cpu in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (1.11.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from faiss-cpu) (2.2.1)\n",
            "Requirement already satisfied: packaging in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from faiss-cpu) (25.0)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: gradio in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (5.36.2)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: audioop-lts<1.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.2.1)\n",
            "Requirement already satisfied: brotli>=1.1.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.4 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (1.10.4)\n",
            "Requirement already satisfied: groovy~=0.1 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.33.4)\n",
            "Requirement already satisfied: jinja2<4.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (2.2.1)\n",
            "Requirement already satisfied: orjson~=3.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (2.2.3)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (2.11.5)\n",
            "Requirement already satisfied: pydub in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (4.14.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio-client==1.10.4->gradio) (2025.5.1)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from gradio-client==1.10.4->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from typer<1.0,>=0.12->gradio) (13.7.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
            "Requirement already satisfied: filelock in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pypdf in c:\\users\\sawsa\\appdata\\roaming\\python\\python313\\site-packages (5.7.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu\n",
        "!pip install gradio\n",
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HiCBSvEZGiu"
      },
      "source": [
        "### üì¶ Part 1: Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqaekU9wSrNz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pypdf import PdfReader\n",
        "import warnings\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# Suppress specific warnings if needed\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "print(\"Libraries imported successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZQo9lnoZX10"
      },
      "source": [
        "### ‚öôÔ∏è Part 2: Configuration ‚Äì Device, Model Selection, and Role Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJqeYLPPVEbv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CPU. Note: LLM inference will be significantly slower.\n"
          ]
        }
      ],
      "source": [
        "# --- Configuration ---\n",
        "# Select device (GPU if available, otherwise CPU)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU. Note: LLM inference will be significantly slower.\")\n",
        "\n",
        "# --- Model Selection ---\n",
        "EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'\n",
        "LLM_MODEL_NAME = \"google/gemma-2b-it\"\n",
        "TARGET_LANGUAGE = \"Finnish\"\n",
        "\n",
        "# --- Role Definitions ---\n",
        "ROLES = {\n",
        "    \"Teacher\": {\n",
        "        \"description\": \"Explains concepts clearly with examples, uses pedagogical approach\",\n",
        "        \"emoji\": \"üë©‚Äçüè´\",\n",
        "        \"traits\": \"patient, educational, uses analogies and examples\"\n",
        "    },\n",
        "    \"Expert Reviewer\": {\n",
        "        \"description\": \"Provides critical analysis and detailed evaluation\",\n",
        "        \"emoji\": \"üîç\",\n",
        "        \"traits\": \"analytical, thorough, objective, professional\"\n",
        "    },\n",
        "    \"Legal Advisor\": {\n",
        "        \"description\": \"Formal language with cautious interpretation\",\n",
        "        \"emoji\": \"‚öñÔ∏è\",\n",
        "        \"traits\": \"formal, precise, includes disclaimers, risk-aware\"\n",
        "    },\n",
        "    \"Technical Writer\": {\n",
        "        \"description\": \"Structured and precise documentation-style responses\",\n",
        "        \"emoji\": \"üíª\",\n",
        "        \"traits\": \"structured, clear, technical, uses bullet points and sections\"\n",
        "    },\n",
        "    \"Friendly Assistant\": {\n",
        "        \"description\": \"Conversational and approachable tone\",\n",
        "        \"emoji\": \"üòä\",\n",
        "        \"traits\": \"warm, helpful, conversational, encouraging\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- Global Variables ---\n",
        "embedder = None\n",
        "text_generator = None\n",
        "tokenizer = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHE1Wvo-Zh91"
      },
      "source": [
        "### üîë Part 3: Authenticate with Hugging Face Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9FgmpSfVzIn"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'huggingface-cli' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgSiV3buZyIM"
      },
      "source": [
        "### ü§ñ Part 4: Load Models ‚Äì Embedder & LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QdeLg10VRHs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading embedding model: all-MiniLM-L6-v2...\n",
            "Embedding model loaded successfully.\n",
            "Loading LLM: google/gemma-2b-it...\n",
            "Error loading LLM or creating pipeline: You are trying to access a gated repo.\n",
            "Make sure to have access to it at https://huggingface.co/google/gemma-2b-it.\n",
            "401 Client Error. (Request ID: Root=1-687b9589-28638f7861a198101b394294;cf68958a-4482-49e4-8ed3-f66bfc729d48)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-2b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-2b-it is restricted. You must have access to it and be authenticated to access it. Please log in.\n",
            "Failed to load models. Error: You are trying to access a gated repo.\n",
            "Make sure to have access to it at https://huggingface.co/google/gemma-2b-it.\n",
            "401 Client Error. (Request ID: Root=1-687b9589-28638f7861a198101b394294;cf68958a-4482-49e4-8ed3-f66bfc729d48)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/google/gemma-2b-it/resolve/main/config.json.\n",
            "Access to model google/gemma-2b-it is restricted. You must have access to it and be authenticated to access it. Please log in.\n"
          ]
        }
      ],
      "source": [
        "def load_models():\n",
        "    \"\"\"Loads the embedding and language models.\"\"\"\n",
        "    global embedder, text_generator, tokenizer\n",
        "\n",
        "    print(f\"Loading embedding model: {EMBEDDING_MODEL_NAME}...\")\n",
        "    try:\n",
        "        embedder = SentenceTransformer(EMBEDDING_MODEL_NAME, device=device)\n",
        "        print(\"Embedding model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading embedding model: {e}\")\n",
        "        raise\n",
        "\n",
        "    print(f\"Loading LLM: {LLM_MODEL_NAME}...\")\n",
        "    try:\n",
        "        use_4bit = True \n",
        "        bnb_config = None\n",
        "        if use_4bit and torch.cuda.is_available():\n",
        "            try:\n",
        "                bnb_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_quant_type=\"nf4\",\n",
        "                    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "                    bnb_4bit_use_double_quant=False,\n",
        "                )\n",
        "                print(\"Using 4-bit quantization.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Could not set up 4-bit quantization, proceeding without it: {e}\")\n",
        "                bnb_config = None\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
        "\n",
        "        if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
        "             model_dtype = torch.bfloat16\n",
        "             print(\"Using bfloat16 dtype.\")\n",
        "        else:\n",
        "             model_dtype = torch.float16\n",
        "             print(\"Using float16 dtype.\")\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            LLM_MODEL_NAME,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=model_dtype,\n",
        "            quantization_config=bnb_config,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        print(\"LLM loaded successfully.\")\n",
        "\n",
        "        text_generator = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            max_new_tokens=400,  # Increased for role-based responses\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            framework=\"pt\"\n",
        "        )\n",
        "        print(\"Text generation pipeline ready.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading LLM or creating pipeline: {e}\")\n",
        "        raise\n",
        "\n",
        "# Load models\n",
        "try:\n",
        "    load_models()\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load models. Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gfVsNA_aJeI"
      },
      "source": [
        "### üìÑ Part 5: Document Processing & Vector Store Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ga8c62D-W-kG"
      },
      "outputs": [],
      "source": [
        "def load_and_chunk_pdf(file_path, chunk_size=700, chunk_overlap=70):\n",
        "    \"\"\"\n",
        "    Loads text from a PDF file, cleans it, and splits it into overlapping chunks.\n",
        "    \"\"\"\n",
        "    if not file_path or not os.path.exists(file_path):\n",
        "        print(f\"Error: PDF file not found at {file_path}\")\n",
        "        return None\n",
        "    try:\n",
        "        print(f\"Loading PDF: {file_path}\")\n",
        "        reader = PdfReader(file_path)\n",
        "        text = \"\"\n",
        "        for i, page in enumerate(reader.pages):\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \"\\n\"\n",
        "            else:\n",
        "                print(f\"Warning: Could not extract text from page {i+1}.\")\n",
        "\n",
        "        if not text:\n",
        "            print(\"Error: No text extracted from the PDF.\")\n",
        "            return None\n",
        "\n",
        "        # Basic cleaning\n",
        "        text = re.sub(r'\\n\\s*\\n', '\\n', text).strip()\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Simple sliding window chunking\n",
        "        chunks = []\n",
        "        start_index = 0\n",
        "        while start_index < len(text):\n",
        "            end_index = start_index + chunk_size\n",
        "            chunks.append(text[start_index:end_index])\n",
        "            start_index += chunk_size - chunk_overlap\n",
        "\n",
        "        chunks = [chunk for chunk in chunks if len(chunk.strip()) > 50]\n",
        "\n",
        "        print(f\"Document loaded and split into {len(chunks)} chunks.\")\n",
        "        return chunks\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or chunking PDF '{file_path}': {e}\")\n",
        "        return None\n",
        "\n",
        "def build_vector_store(chunks, embedder_model):\n",
        "    \"\"\"\n",
        "    Generates embeddings for text chunks and builds a FAISS index.\n",
        "    \"\"\"\n",
        "    if not chunks or embedder_model is None:\n",
        "        print(\"Error: No chunks or embedder model provided.\")\n",
        "        return None, None\n",
        "    try:\n",
        "        print(f\"Generating embeddings for {len(chunks)} chunks...\")\n",
        "        embeddings = embedder_model.encode(chunks, convert_to_tensor=False, show_progress_bar=True)\n",
        "        embeddings_np = np.array(embeddings).astype('float32')\n",
        "\n",
        "        embedding_dim = embeddings_np.shape[1]\n",
        "        index = faiss.IndexFlatL2(embedding_dim)\n",
        "        index.add(embeddings_np)\n",
        "\n",
        "        print(f\"FAISS index created successfully with {index.ntotal} vectors.\")\n",
        "        return index, chunks\n",
        "    except Exception as e:\n",
        "        print(f\"Error building vector store: {e}\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUZ17bFcaW_A"
      },
      "source": [
        "### üîé Part 6: Semantic Retrieval from Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkCsDmstXB8A"
      },
      "outputs": [],
      "source": [
        "def retrieve_context(query, vector_store, embedder_model, indexed_chunks, top_k=3):\n",
        "    \"\"\"\n",
        "    Retrieves the top_k most relevant text chunks from the vector store.\n",
        "    \"\"\"\n",
        "    if vector_store is None or embedder_model is None or indexed_chunks is None:\n",
        "        return \"Error: Vector store, embedder, or chunks not initialized.\"\n",
        "    try:\n",
        "        print(f\"Retrieving context for query: '{query}'\")\n",
        "        query_embedding = embedder_model.encode([query], convert_to_tensor=False)\n",
        "        query_embedding_np = np.array(query_embedding).astype('float32')\n",
        "\n",
        "        distances, indices = vector_store.search(query_embedding_np, top_k)\n",
        "        retrieved_chunks = [indexed_chunks[i] for i in indices[0] if 0 <= i < len(indexed_chunks)]\n",
        "\n",
        "        if not retrieved_chunks:\n",
        "            print(\"Warning: No relevant chunks found for the query.\")\n",
        "            return \"Could not find relevant context for this query in the document.\"\n",
        "\n",
        "        context = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
        "        print(f\"Retrieved {len(retrieved_chunks)} chunks.\")\n",
        "        return context\n",
        "    except Exception as e:\n",
        "        print(f\"Error during context retrieval: {e}\")\n",
        "        return f\"Error retrieving context: {e}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9o6v8o_au9Q"
      },
      "source": [
        "### üé≠ Part 7: Role-Based Prompt Templates\n",
        "\n",
        "This is the core featured - role-specific prompts that dramatically change how the AI responds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYATX2YCSY7V"
      },
      "outputs": [],
      "source": [
        "# Role-Based Prompt Templates\n",
        "\n",
        "def get_role_based_qa_prompt(role: str) -> str:\n",
        "    \"\"\"Generate role-specific QA prompt templates.\"\"\"\n",
        "    \n",
        "    base_instruction = \"\"\"Use the following context to answer the question. \n",
        "If the answer is not found in the context, state that you cannot answer based on the provided information.\"\"\"\n",
        "    \n",
        "    role_templates = {\n",
        "        \"Teacher\": f\"\"\"SYSTEM: You are a knowledgeable teacher who explains concepts clearly. {base_instruction}\n",
        "When answering:\n",
        "- Break down complex ideas into simple terms\n",
        "- Use analogies and examples where helpful\n",
        "- Encourage understanding with a patient, educational tone\n",
        "- If the concept is difficult, provide step-by-step explanations\n",
        "\n",
        "CONTEXT:\n",
        "{{context}}\n",
        "\n",
        "STUDENT'S QUESTION: {{query}}\n",
        "\n",
        "TEACHER'S RESPONSE:\"\"\",\n",
        "        \n",
        "        \"Expert Reviewer\": f\"\"\"SYSTEM: You are an expert reviewer providing critical analysis. {base_instruction}\n",
        "When answering:\n",
        "- Provide thorough, analytical responses\n",
        "- Evaluate claims critically\n",
        "- Identify strengths and potential weaknesses\n",
        "- Maintain objectivity and professionalism\n",
        "\n",
        "CONTEXT:\n",
        "{{context}}\n",
        "\n",
        "QUESTION FOR REVIEW: {{query}}\n",
        "\n",
        "EXPERT ANALYSIS:\"\"\",\n",
        "        \n",
        "        \"Legal Advisor\": f\"\"\"SYSTEM: You are a legal advisor providing formal guidance. {base_instruction}\n",
        "Important: You are not providing actual legal advice, only information based on the document.\n",
        "When answering:\n",
        "- Use precise, formal language\n",
        "- Include appropriate disclaimers\n",
        "- Be cautious in interpretation\n",
        "- Highlight any assumptions or limitations\n",
        "\n",
        "CONTEXT:\n",
        "{{context}}\n",
        "\n",
        "INQUIRY: {{query}}\n",
        "\n",
        "LEGAL INTERPRETATION (For informational purposes only):\"\"\",\n",
        "        \n",
        "        \"Technical Writer\": f\"\"\"SYSTEM: You are a technical writer creating clear documentation. {base_instruction}\n",
        "When answering:\n",
        "- Use structured, precise language\n",
        "- Include bullet points or numbered lists where appropriate\n",
        "- Define technical terms clearly\n",
        "- Maintain consistency in terminology\n",
        "\n",
        "CONTEXT:\n",
        "{{context}}\n",
        "\n",
        "TECHNICAL QUERY: {{query}}\n",
        "\n",
        "TECHNICAL RESPONSE:\"\"\",\n",
        "        \n",
        "        \"Friendly Assistant\": f\"\"\"SYSTEM: You are a friendly, helpful assistant. {base_instruction}\n",
        "When answering:\n",
        "- Use a warm, conversational tone\n",
        "- Be encouraging and supportive\n",
        "- Make the information accessible\n",
        "- Show enthusiasm for helping\n",
        "\n",
        "CONTEXT:\n",
        "{{context}}\n",
        "\n",
        "USER'S QUESTION: {{query}}\n",
        "\n",
        "FRIENDLY RESPONSE:\"\"\"\n",
        "    }\n",
        "    \n",
        "    return role_templates.get(role, role_templates[\"Friendly Assistant\"])\n",
        "\n",
        "\n",
        "def get_role_based_summary_prompt(role: str) -> str:\n",
        "    \"\"\"Generate role-specific summarization prompt templates.\"\"\"\n",
        "    \n",
        "    role_templates = {\n",
        "        \"Teacher\": \"\"\"SYSTEM: You are a teacher creating an educational summary. Based on the following text, provide a summary that:\n",
        "- Highlights key learning points\n",
        "- Organizes information for easy understanding\n",
        "- Includes main concepts and their relationships\n",
        "- Uses clear, educational language\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "EDUCATIONAL SUMMARY:\"\"\",\n",
        "        \n",
        "        \"Expert Reviewer\": \"\"\"SYSTEM: You are an expert reviewer creating a critical summary. Based on the following text, provide a summary that:\n",
        "- Identifies main arguments and evidence\n",
        "- Evaluates the strength of claims\n",
        "- Notes any gaps or limitations\n",
        "- Maintains analytical objectivity\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "CRITICAL SUMMARY:\"\"\",\n",
        "        \n",
        "        \"Legal Advisor\": \"\"\"SYSTEM: You are a legal advisor creating a formal summary. Based on the following text, provide a summary that:\n",
        "- Identifies key legal or formal points\n",
        "- Uses precise, formal language\n",
        "- Notes important definitions or conditions\n",
        "- Maintains cautious interpretation\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "FORMAL SUMMARY:\"\"\",\n",
        "        \n",
        "        \"Technical Writer\": \"\"\"SYSTEM: You are a technical writer creating a structured summary. Based on the following text, provide a summary that:\n",
        "- Uses clear section headings\n",
        "- Includes key technical details\n",
        "- Maintains consistent terminology\n",
        "- Organizes information logically\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "TECHNICAL SUMMARY:\"\"\",\n",
        "        \n",
        "        \"Friendly Assistant\": \"\"\"SYSTEM: You are a friendly assistant creating an accessible summary. Based on the following text, provide a summary that:\n",
        "- Makes complex ideas simple\n",
        "- Uses conversational language\n",
        "- Highlights the most interesting points\n",
        "- Keeps the reader engaged\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "FRIENDLY SUMMARY:\"\"\"\n",
        "    }\n",
        "    \n",
        "    return role_templates.get(role, role_templates[\"Friendly Assistant\"])\n",
        "\n",
        "\n",
        "def get_role_based_translation_prompt(role: str, target_language: str) -> str:\n",
        "    \"\"\"Generate role-specific translation prompt templates.\"\"\"\n",
        "    \n",
        "    role_templates = {\n",
        "        \"Teacher\": f\"\"\"SYSTEM: You are a language teacher translating for educational purposes. Translate the following text into {target_language}:\n",
        "- Maintain clarity for learners\n",
        "- Preserve educational value\n",
        "- Keep explanations simple\n",
        "\n",
        "CONTEXT:\n",
        "{{context}}\n",
        "\n",
        "EDUCATIONAL TRANSLATION ({target_language}):\"\"\",\n",
        "        \n",
        "        \"Expert Reviewer\": f\"\"\"SYSTEM: You are an expert translator focusing on accuracy. Translate the following text into {target_language}:\n",
        "- Maintain technical precision\n",
        "- Preserve nuanced meanings\n",
        "- Keep professional terminology\n",
        "\n",
        "CONTEXT:\n",
        "{{context}}\n",
        "\n",
        "PROFESSIONAL TRANSLATION ({target_language}):\"\"\",\n",
        "        \n",
        "        \"Legal Advisor\": f\"\"\"SYSTEM: You are translating legal/formal content. Translate the following text into {target_language}:\n",
        "- Maintain formal register\n",
        "- Preserve legal precision\n",
        "- Keep official terminology\n",
        "\n",
        "CONTEXT:\n",
        "{{context}}\n",
        "\n",
        "FORMAL TRANSLATION ({target_language}):\"\"\",\n",
        "        \n",
        "        \"Technical Writer\": f\"\"\"SYSTEM: You are translating technical documentation. Translate the following text into {target_language}:\n",
        "- Maintain technical accuracy\n",
        "- Keep consistent terminology\n",
        "- Preserve structure\n",
        "\n",
        "CONTEXT:\n",
        "{{context}}\n",
        "\n",
        "TECHNICAL TRANSLATION ({target_language}):\"\"\",\n",
        "        \n",
        "        \"Friendly Assistant\": f\"\"\"SYSTEM: You are creating a friendly, natural translation. Translate the following text into {target_language}:\n",
        "- Use natural, conversational language\n",
        "- Make it sound native\n",
        "- Keep the friendly tone\n",
        "\n",
        "CONTEXT:\n",
        "{{context}}\n",
        "\n",
        "FRIENDLY TRANSLATION ({target_language}):\"\"\"\n",
        "    }\n",
        "    \n",
        "    return role_templates.get(role, role_templates[\"Friendly Assistant\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enhanced_generation"
      },
      "source": [
        "### üß† Part 8: Response Generation with Role Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enhanced_gen_code"
      },
      "outputs": [],
      "source": [
        "def generate_response(query, context, task, role, generator_pipeline):\n",
        "    \"\"\"\n",
        "    Generates a response using role-specific prompts.\n",
        "    \"\"\"\n",
        "    if generator_pipeline is None:\n",
        "        return \"Error: Text generation pipeline not initialized.\"\n",
        "    \n",
        "    if not context or \"Error:\" in context or \"Could not find relevant context\" in context:\n",
        "        return f\"Cannot generate response because context retrieval failed: {context}\"\n",
        "\n",
        "    try:\n",
        "        # Select appropriate prompt template based on task and role\n",
        "        if task == \"Ask a question\":\n",
        "            prompt_template = get_role_based_qa_prompt(role)\n",
        "            prompt = prompt_template.format(context=context, query=query)\n",
        "        elif task == \"Summarize\":\n",
        "            prompt_template = get_role_based_summary_prompt(role)\n",
        "            prompt = prompt_template.format(context=context)\n",
        "        elif task == f\"Translate (to {TARGET_LANGUAGE})\":\n",
        "            prompt_template = get_role_based_translation_prompt(role, TARGET_LANGUAGE)\n",
        "            prompt = prompt_template.format(context=context)\n",
        "        else:\n",
        "            return \"Error: Invalid task selected.\"\n",
        "\n",
        "        print(f\"Generating response with LLM using role: {role}...\")\n",
        "        outputs = generator_pipeline(prompt)\n",
        "        generated_text = outputs[0]['generated_text']\n",
        "\n",
        "        # Clean up the response based on role markers\n",
        "        role_markers = [\n",
        "            \"TEACHER'S RESPONSE:\", \"EXPERT ANALYSIS:\", \"LEGAL INTERPRETATION\",\n",
        "            \"TECHNICAL RESPONSE:\", \"FRIENDLY RESPONSE:\", \"EDUCATIONAL SUMMARY:\",\n",
        "            \"CRITICAL SUMMARY:\", \"FORMAL SUMMARY:\", \"TECHNICAL SUMMARY:\",\n",
        "            \"FRIENDLY SUMMARY:\", f\"EDUCATIONAL TRANSLATION ({TARGET_LANGUAGE}):\",\n",
        "            f\"PROFESSIONAL TRANSLATION ({TARGET_LANGUAGE}):\",\n",
        "            f\"FORMAL TRANSLATION ({TARGET_LANGUAGE}):\",\n",
        "            f\"TECHNICAL TRANSLATION ({TARGET_LANGUAGE}):\",\n",
        "            f\"FRIENDLY TRANSLATION ({TARGET_LANGUAGE}):\"\n",
        "        ]\n",
        "        \n",
        "        response = generated_text\n",
        "        for marker in role_markers:\n",
        "            if marker in response:\n",
        "                response = response.split(marker)[-1].strip()\n",
        "                break\n",
        "\n",
        "        print(\"LLM Generation complete.\")\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during LLM generation: {e}\")\n",
        "        if \"CUDA out of memory\" in str(e):\n",
        "            return \"Error: GPU out of memory. Try a smaller model or enable 4-bit quantization.\"\n",
        "        return f\"Error generating response: {e}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tabx-z0sbg6c"
      },
      "source": [
        "### üé≠ Part 9: Gradio Interface with Role Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUUzwPTSbZfF"
      },
      "outputs": [],
      "source": [
        "# Gradio Interface Logic with Role Selection\n",
        "\n",
        "# Store vector store state\n",
        "document_state = {\n",
        "    \"file_path\": None,\n",
        "    \"vector_store\": None,\n",
        "    \"indexed_chunks\": None\n",
        "}\n",
        "\n",
        "def process_document_and_query(file_obj, task, query, role):\n",
        "    \"\"\"\n",
        "    Main function with role-based processing.\n",
        "    \"\"\"\n",
        "    global document_state\n",
        "\n",
        "    status_message = \"\"\n",
        "    result_output = \"\"\n",
        "\n",
        "    # Check Models\n",
        "    if embedder is None or text_generator is None:\n",
        "        status_message = \"Error: Models not loaded. Please check console.\"\n",
        "        print(status_message)\n",
        "        return status_message, result_output\n",
        "\n",
        "    # Process Document\n",
        "    current_file_path = file_obj.name if file_obj else None\n",
        "\n",
        "    if current_file_path is None:\n",
        "        status_message = \"Please upload a PDF document.\"\n",
        "        return status_message, result_output\n",
        "\n",
        "    if current_file_path != document_state.get(\"file_path\"):\n",
        "        status_message = f\"Processing new document: {os.path.basename(current_file_path)}...\"\n",
        "        print(status_message)\n",
        "        document_state = {\"file_path\": None, \"vector_store\": None, \"indexed_chunks\": None}\n",
        "\n",
        "        chunks = load_and_chunk_pdf(current_file_path)\n",
        "        if chunks is None:\n",
        "            status_message = \"Error: Failed to load or chunk the PDF.\"\n",
        "            print(status_message)\n",
        "            return status_message, result_output\n",
        "\n",
        "        vector_store, indexed_chunks = build_vector_store(chunks, embedder)\n",
        "        if vector_store is None:\n",
        "            status_message = \"Error: Failed to build the vector store.\"\n",
        "            print(status_message)\n",
        "            return status_message, result_output\n",
        "\n",
        "        document_state[\"file_path\"] = current_file_path\n",
        "        document_state[\"vector_store\"] = vector_store\n",
        "        document_state[\"indexed_chunks\"] = indexed_chunks\n",
        "        status_message = f\"Document processed successfully. Using role: {ROLES[role]['emoji']} {role}\"\n",
        "        print(status_message)\n",
        "    else:\n",
        "        status_message = f\"Using cached document. Role: {ROLES[role]['emoji']} {role}\"\n",
        "        print(status_message)\n",
        "        vector_store = document_state[\"vector_store\"]\n",
        "        indexed_chunks = document_state[\"indexed_chunks\"]\n",
        "\n",
        "    # Perform Selected Task with Role\n",
        "    print(f\"Task: {task}, Role: {role}\")\n",
        "\n",
        "    # Set retrieval parameters based on task\n",
        "    if task == \"Ask a question\":\n",
        "        if not query:\n",
        "            status_message += \"\\nPlease enter a question.\"\n",
        "            return status_message, result_output\n",
        "        context_query = query\n",
        "        top_k = 3\n",
        "    elif task == \"Summarize\":\n",
        "        context_query = \"Provide a comprehensive overview of the document's content.\"\n",
        "        top_k = 6\n",
        "    elif task == f\"Translate (to {TARGET_LANGUAGE})\":\n",
        "        if not query:\n",
        "            if not indexed_chunks:\n",
        "                status_message += \"\\nError: Cannot translate without document content.\"\n",
        "                return status_message, result_output\n",
        "            context_query = indexed_chunks[0][:150] + \"...\"\n",
        "        else:\n",
        "            context_query = query\n",
        "        top_k = 3\n",
        "    else:\n",
        "        status_message += \"\\nError: Invalid task selected.\"\n",
        "        return status_message, result_output\n",
        "\n",
        "    # Retrieve context\n",
        "    context = retrieve_context(context_query, vector_store, embedder, indexed_chunks, top_k=top_k)\n",
        "\n",
        "    # Generate response with role\n",
        "    if \"Error:\" in context or \"Could not find relevant context\" in context:\n",
        "        result_output = f\"Failed to retrieve context: {context}\"\n",
        "    else:\n",
        "        result_output = generate_response(query, context, task, role, text_generator)\n",
        "\n",
        "    status_message += f\"\\nTask '{task}' completed with {role} persona.\"\n",
        "    print(status_message)\n",
        "\n",
        "    return status_message, result_output\n",
        "\n",
        "\n",
        "def get_role_info(role):\n",
        "    \"\"\"Provide information about the selected role.\"\"\"\n",
        "    if role in ROLES:\n",
        "        info = ROLES[role]\n",
        "        return f\"{info['emoji']} **{role}**: {info['description']}\\n\\n*Traits: {info['traits']}*\"\n",
        "    return \"Select a role to see its description.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnmiXQuwbQK5"
      },
      "source": [
        "### üñ•Ô∏è Part 10: Create and Launch the Gradio Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSStb-I9bFv5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up Enhanced Gradio interface with Role-Based Prompting...\n",
            "Launching Enhanced Gradio interface...\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced interface with role-based prompting is ready!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Models not loaded. Please check console.\n"
          ]
        }
      ],
      "source": [
        "# Create and Launch the Gradio Interface\n",
        "\n",
        "print(\"Setting up Gradio interface with Role-Based Prompting...\")\n",
        "\n",
        "# Custom CSS for better UI\n",
        "custom_css = \"\"\"\n",
        ".role-info {\n",
        "    background-color: #f0f0f0;\n",
        "    padding: 10px;\n",
        "    border-radius: 5px;\n",
        "    margin: 10px 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Create the interface\n",
        "with gr.Blocks(css=custom_css, title=\"Smart AI with Role-Based Prompting\") as iface:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # üé≠ Smart Document Helper with Role-Based AI\n",
        "        \n",
        "        Upload a PDF and let our AI assist you with different personas!\n",
        "        Each role provides a unique perspective and communication style.\n",
        "        \"\"\"\n",
        "    )\n",
        "    \n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            file_input = gr.File(\n",
        "                label=\"üìÑ Upload PDF Document\", \n",
        "                file_types=[\".pdf\"]\n",
        "            )\n",
        "            \n",
        "            role_dropdown = gr.Dropdown(\n",
        "                label=\"üé≠ Select AI Role\",\n",
        "                choices=list(ROLES.keys()),\n",
        "                value=\"Friendly Assistant\",\n",
        "                info=\"Choose how the AI should respond\"\n",
        "            )\n",
        "            \n",
        "            role_info = gr.Markdown(\n",
        "                value=get_role_info(\"Friendly Assistant\"),\n",
        "                elem_classes=[\"role-info\"]\n",
        "            )\n",
        "            \n",
        "            task_dropdown = gr.Dropdown(\n",
        "                label=\"üìã Select Task\",\n",
        "                choices=[\"Ask a question\", \"Summarize\", f\"Translate (to {TARGET_LANGUAGE})\"],\n",
        "                value=\"Ask a question\"\n",
        "            )\n",
        "            \n",
        "            query_input = gr.Textbox(\n",
        "                label=\"‚ùì Enter Question or Topic\",\n",
        "                placeholder=\"What would you like to know about the document?\",\n",
        "                lines=3\n",
        "            )\n",
        "            \n",
        "            submit_btn = gr.Button(\"üöÄ Process\", variant=\"primary\")\n",
        "        \n",
        "        with gr.Column(scale=2):\n",
        "            status_output = gr.Textbox(\n",
        "                label=\"üìä Status\", \n",
        "                interactive=False,\n",
        "                lines=2\n",
        "            )\n",
        "            \n",
        "            result_output = gr.Textbox(\n",
        "                label=\"üí¨ AI Response\", \n",
        "                lines=20, \n",
        "                interactive=False\n",
        "            )\n",
        "    \n",
        "    # Examples section\n",
        "    gr.Markdown(\"### üìö Example Queries\")\n",
        "    gr.Examples(\n",
        "        examples=[\n",
        "            [None, \"Ask a question\", \"What is the main purpose of this document?\", \"Teacher\"],\n",
        "            [None, \"Ask a question\", \"What are the key findings?\", \"Expert Reviewer\"],\n",
        "            [None, \"Summarize\", \"\", \"Technical Writer\"],\n",
        "            [None, \"Summarize\", \"\", \"Legal Advisor\"],\n",
        "            [None, f\"Translate (to {TARGET_LANGUAGE})\", \"Introduction section\", \"Friendly Assistant\"]\n",
        "        ],\n",
        "        inputs=[file_input, task_dropdown, query_input, role_dropdown],\n",
        "        cache_examples=False\n",
        "    )\n",
        "    \n",
        "    # Event handlers\n",
        "    role_dropdown.change(\n",
        "        fn=get_role_info,\n",
        "        inputs=[role_dropdown],\n",
        "        outputs=[role_info]\n",
        "    )\n",
        "    \n",
        "    submit_btn.click(\n",
        "        fn=process_document_and_query,\n",
        "        inputs=[file_input, task_dropdown, query_input, role_dropdown],\n",
        "        outputs=[status_output, result_output]\n",
        "    )\n",
        "    \n",
        "    gr.Markdown(\n",
        "        f\"\"\"\n",
        "        ---\n",
        "        ### üõ†Ô∏è Technical Details\n",
        "        - **Embedding Model**: {EMBEDDING_MODEL_NAME}\n",
        "        - **Language Model**: {LLM_MODEL_NAME}\n",
        "        - **Extended**: Role-Based Prompting with 5 distinct personas\n",
        "        - **Features**: Dynamic prompt adaptation based on selected role\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "print(\"Launching Gradio interface...\")\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch(debug=False, share=True)\n",
        "\n",
        "print(\"Interface with role-based prompting is ready!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
