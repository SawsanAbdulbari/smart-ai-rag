{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_WFbYH_YYOt"
   },
   "source": [
    "### üöÄ Ultra-Smart AI - Multi-Strategy RAG System\n",
    "\n",
    "This advanced version combines **multiple prompting strategies** for superior performance:\n",
    "\n",
    "**üéØ Combined Strategies:**\n",
    "\n",
    "1. **üé≠ Role-Based Prompting**: Choose from multiple AI personas\n",
    "2. **üìö Few-Shot Learning**: Provide examples to guide responses\n",
    "3. **üß† Chain-of-Thought**: Step-by-step reasoning for complex queries\n",
    "4. **‚úèÔ∏è Prompt Editing**: Live prompt customization\n",
    "5. **üîÑ Self-Consistency**: Multiple response generation with voting\n",
    "\n",
    "**‚ú® Interactive Features:**\n",
    "\n",
    "- **Example Library**: Pre-loaded examples for each role\n",
    "- **Prompt Preview**: See and edit the actual prompt\n",
    "- **Response Comparison**: Compare outputs from different strategies\n",
    "- **Confidence Scoring**: AI self-assessment of response quality\n",
    "- **Strategy Mixing**: Combine multiple strategies dynamically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--f_ZlmLYw5O"
   },
   "source": [
    "### üîß Part 0: Setup and Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Q6G7oY16S2e5"
   },
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu\n",
    "# !pip install gradio\n",
    "# !pip install pypdf\n",
    "# !pip install plotly  # For confidence visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HiCBSvEZGiu"
   },
   "source": [
    "### üì¶ Part 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vqaekU9wSrNz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pypdf import PdfReader\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZQo9lnoZX10"
   },
   "source": [
    "### ‚öôÔ∏è Part 2: Configuration with Example Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qJqeYLPPVEbv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU. Note: LLM inference will be significantly slower.\")\n",
    "\n",
    "# --- Model Selection ---\n",
    "EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "LLM_MODEL_NAME = \"google/gemma-2b-it\" \n",
    "TARGET_LANGUAGE = \"Finnish\"\n",
    "\n",
    "# ---  Role Definitions with Examples ---\n",
    "@dataclass\n",
    "class RoleConfig:\n",
    "    name: str\n",
    "    description: str\n",
    "    emoji: str\n",
    "    traits: str\n",
    "    few_shot_examples: List[Dict[str, str]]\n",
    "    thinking_style: str\n",
    "\n",
    "ROLES = {\n",
    "    \"Teacher\": RoleConfig(\n",
    "        name=\"Teacher\",\n",
    "        description=\"Explains concepts clearly with examples\",\n",
    "        emoji=\"üë©‚Äçüè´\",\n",
    "        traits=\"patient, educational, uses analogies and examples\",\n",
    "        thinking_style=\"Let me think about how to explain this clearly...\",\n",
    "        few_shot_examples=[\n",
    "            {\n",
    "                \"query\": \"What is machine learning?\",\n",
    "                \"response\": \"Great question! Machine learning is like teaching a computer to learn from experience, just like how you learned to ride a bike. At first, you might fall, but with practice (data), you get better. The computer looks at many examples and finds patterns, so it can make good guesses about new things it hasn't seen before!\"\n",
    "            },\n",
    "            {\n",
    "                \"query\": \"How does photosynthesis work?\",\n",
    "                \"response\": \"Think of plants as little solar-powered factories! They use sunlight like we use electricity. The leaves capture sunlight (like solar panels), mix it with water from the roots and CO2 from the air, and create food (glucose) for the plant. As a bonus, they release oxygen for us to breathe! It's nature's perfect recycling system.\"\n",
    "            }\n",
    "        ]\n",
    "    ),\n",
    "    \"Expert Reviewer\": RoleConfig(\n",
    "        name=\"Expert Reviewer\",\n",
    "        description=\"Provides critical analysis and detailed evaluation\",\n",
    "        emoji=\"üîç\",\n",
    "        traits=\"analytical, thorough, objective, professional\",\n",
    "        thinking_style=\"Let me analyze this systematically...\",\n",
    "        few_shot_examples=[\n",
    "            {\n",
    "                \"query\": \"Evaluate this claim: 'AI will replace all jobs'\",\n",
    "                \"response\": \"This claim requires nuanced analysis. While AI demonstrates transformative potential in automating routine tasks (data entry, basic analysis), historical precedent suggests technology creates new job categories while eliminating others. Key considerations: 1) Creative and interpersonal roles remain human-dominant, 2) AI augmentation vs. replacement paradigm, 3) Reskilling timeframes. The claim oversimplifies a complex socioeconomic transition.\"\n",
    "            },\n",
    "            {\n",
    "                \"query\": \"What are the limitations of this study?\",\n",
    "                \"response\": \"Several methodological concerns warrant attention: 1) Sample size (n=50) limits generalizability, 2) Self-reported data introduces response bias, 3) Lack of control group prevents causal inference, 4) Two-month duration insufficient for long-term effects. Additionally, demographic homogeneity (urban, 25-35 age range) restricts external validity. Recommend replication with larger, diverse cohorts.\"\n",
    "            }\n",
    "        ]\n",
    "    ),\n",
    "    \"Technical Writer\": RoleConfig(\n",
    "        name=\"Technical Writer\",\n",
    "        description=\"Structured and precise documentation-style responses\",\n",
    "        emoji=\"üíª\",\n",
    "        traits=\"structured, clear, technical, uses bullet points and sections\",\n",
    "        thinking_style=\"Let me structure this information clearly...\",\n",
    "        few_shot_examples=[\n",
    "            {\n",
    "                \"query\": \"How to set up a web server?\",\n",
    "                \"response\": \"## Web Server Setup Guide\\n\\n### Prerequisites\\n- Operating System: Linux/Windows/macOS\\n- Root/Admin access\\n- Port 80/443 available\\n\\n### Steps\\n1. **Install Web Server Software**\\n   ```bash\\n   sudo apt-get install apache2  # For Apache\\n   sudo apt-get install nginx     # For Nginx\\n   ```\\n\\n2. **Configure Firewall**\\n   - Allow HTTP (port 80)\\n   - Allow HTTPS (port 443)\\n\\n3. **Start Service**\\n   ```bash\\n   sudo systemctl start apache2\\n   sudo systemctl enable apache2\\n   ```\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "# --- Prompting Strategies ---\n",
    "STRATEGIES = {\n",
    "    \"standard\": \"Standard prompting without improvements\",\n",
    "    \"few_shot\": \"Include examples to guide the response\",\n",
    "    \"chain_of_thought\": \"Step-by-step reasoning process\",\n",
    "    \"combined\": \"All strategies combined for maximum effectiveness\"\n",
    "}\n",
    "\n",
    "# --- Global Variables ---\n",
    "embedder = None\n",
    "text_generator = None\n",
    "tokenizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHE1Wvo-Zh91"
   },
   "source": [
    "### üîë Part 3: Authenticate with Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "d9FgmpSfVzIn"
   },
   "outputs": [],
   "source": [
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if using vc use the command above in bash "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgSiV3buZyIM"
   },
   "source": [
    "### ü§ñ Part 4: Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3QdeLg10VRHs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2...\n",
      "Embedding model loaded successfully.\n",
      "Loading LLM: google/gemma-2b-it...\n",
      "Using 4-bit quantization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b5ae3caace46758383699331309ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208f9d99859d4e9c90248ff6ce352de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ai\\hamk2ndyear\\Prompt_Engineering_summer\\Smart_AI_(RAG)\\venv312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sawsa\\.cache\\huggingface\\hub\\models--google--gemma-2b-it. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM loaded successfully.\n",
      "Text generation pipeline ready.\n"
     ]
    }
   ],
   "source": [
    "def load_models():\n",
    "    \"\"\"Loads the embedding and language models.\"\"\"\n",
    "    global embedder, text_generator, tokenizer\n",
    "\n",
    "    print(f\"Loading embedding model: {EMBEDDING_MODEL_NAME}...\")\n",
    "    try:\n",
    "        embedder = SentenceTransformer(EMBEDDING_MODEL_NAME, device=device)\n",
    "        print(\"Embedding model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading embedding model: {e}\")\n",
    "        raise\n",
    "\n",
    "    print(f\"Loading LLM: {LLM_MODEL_NAME}...\")\n",
    "    try:\n",
    "        use_4bit = True \n",
    "        bnb_config = None\n",
    "        if use_4bit and torch.cuda.is_available():\n",
    "            try:\n",
    "                bnb_config = BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_quant_type=\"nf4\",\n",
    "                    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                    bnb_4bit_use_double_quant=False,\n",
    "                )\n",
    "                print(\"Using 4-bit quantization.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not set up 4-bit quantization: {e}\")\n",
    "                bnb_config = None\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
    "\n",
    "        if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
    "             model_dtype = torch.bfloat16\n",
    "        else:\n",
    "             model_dtype = torch.float16\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            LLM_MODEL_NAME,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=model_dtype,\n",
    "            quantization_config=bnb_config,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(\"LLM loaded successfully.\")\n",
    "\n",
    "        text_generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=500,  # Increased for complex strategies\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            framework=\"pt\"\n",
    "        )\n",
    "        print(\"Text generation pipeline ready.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading LLM: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load models\n",
    "try:\n",
    "    load_models()\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load models. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gfVsNA_aJeI"
   },
   "source": [
    "### üìÑ Part 5: Document Processing (Same as before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ga8c62D-W-kG"
   },
   "outputs": [],
   "source": [
    "def load_and_chunk_pdf(file_path, chunk_size=700, chunk_overlap=70):\n",
    "    \"\"\"Loads text from a PDF file and chunks it.\"\"\"\n",
    "    if not file_path or not os.path.exists(file_path):\n",
    "        print(f\"Error: PDF file not found at {file_path}\")\n",
    "        return None\n",
    "    try:\n",
    "        print(f\"Loading PDF: {file_path}\")\n",
    "        reader = PdfReader(file_path)\n",
    "        text = \"\"\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "\n",
    "        if not text:\n",
    "            print(\"Error: No text extracted from the PDF.\")\n",
    "            return None\n",
    "\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n', text).strip()\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        chunks = []\n",
    "        start_index = 0\n",
    "        while start_index < len(text):\n",
    "            end_index = start_index + chunk_size\n",
    "            chunks.append(text[start_index:end_index])\n",
    "            start_index += chunk_size - chunk_overlap\n",
    "\n",
    "        chunks = [chunk for chunk in chunks if len(chunk.strip()) > 50]\n",
    "        print(f\"Document loaded and split into {len(chunks)} chunks.\")\n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "def build_vector_store(chunks, embedder_model):\n",
    "    \"\"\"Generates embeddings and builds FAISS index.\"\"\"\n",
    "    if not chunks or embedder_model is None:\n",
    "        return None, None\n",
    "    try:\n",
    "        print(f\"Generating embeddings for {len(chunks)} chunks...\")\n",
    "        embeddings = embedder_model.encode(chunks, convert_to_tensor=False, show_progress_bar=True)\n",
    "        embeddings_np = np.array(embeddings).astype('float32')\n",
    "\n",
    "        embedding_dim = embeddings_np.shape[1]\n",
    "        index = faiss.IndexFlatL2(embedding_dim)\n",
    "        index.add(embeddings_np)\n",
    "\n",
    "        print(f\"FAISS index created with {index.ntotal} vectors.\")\n",
    "        return index, chunks\n",
    "    except Exception as e:\n",
    "        print(f\"Error building vector store: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def retrieve_context(query, vector_store, embedder_model, indexed_chunks, top_k=3):\n",
    "    \"\"\"Retrieves relevant chunks.\"\"\"\n",
    "    if vector_store is None or embedder_model is None or indexed_chunks is None:\n",
    "        return \"Error: Vector store not initialized.\"\n",
    "    try:\n",
    "        query_embedding = embedder_model.encode([query], convert_to_tensor=False)\n",
    "        query_embedding_np = np.array(query_embedding).astype('float32')\n",
    "\n",
    "        distances, indices = vector_store.search(query_embedding_np, top_k)\n",
    "        retrieved_chunks = [indexed_chunks[i] for i in indices[0] if 0 <= i < len(indexed_chunks)]\n",
    "\n",
    "        if not retrieved_chunks:\n",
    "            return \"Could not find relevant context.\"\n",
    "\n",
    "        context = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
    "        return context\n",
    "    except Exception as e:\n",
    "        print(f\"Error during retrieval: {e}\")\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "multi_strategy"
   },
   "source": [
    "### üéØ Part 6: Multi-Strategy Prompt Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "prompt_builder"
   },
   "outputs": [],
   "source": [
    "class PromptBuilder:\n",
    "    \"\"\"Builds prompts with multiple strategies.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_qa_prompt(role: str, query: str, context: str, strategy: str, \n",
    "                       custom_examples: Optional[List[Dict]] = None) -> str:\n",
    "        \"\"\"Build a QA prompt with the selected strategy.\"\"\"\n",
    "        \n",
    "        role_config = ROLES[role]\n",
    "        base_instruction = f\"\"\"You are a {role} with these traits: {role_config.traits}.\n",
    "Use the following context to answer the question.\"\"\"\n",
    "        \n",
    "        if strategy == \"standard\":\n",
    "            return f\"\"\"{base_instruction}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "        \n",
    "        elif strategy == \"few_shot\":\n",
    "            examples = custom_examples or role_config.few_shot_examples\n",
    "            examples_text = \"\\n\\n\".join([\n",
    "                f\"Example {i+1}:\\nQ: {ex['query']}\\nA: {ex['response']}\"\n",
    "                for i, ex in enumerate(examples[:2])  # Use max 2 examples\n",
    "            ])\n",
    "            \n",
    "            return f\"\"\"{base_instruction}\n",
    "\n",
    "Here are some examples of how I respond:\n",
    "\n",
    "{examples_text}\n",
    "\n",
    "Now, using the context provided:\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "        \n",
    "        elif strategy == \"chain_of_thought\":\n",
    "            return f\"\"\"{base_instruction}\n",
    "\n",
    "I'll think through this step-by-step.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "THINKING PROCESS:\n",
    "{role_config.thinking_style}\n",
    "\n",
    "Step 1: Identify the key information in the question\n",
    "Step 2: Find relevant details in the context\n",
    "Step 3: Connect the information logically\n",
    "Step 4: Formulate a clear answer\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "        \n",
    "        elif strategy == \"combined\":\n",
    "            examples = custom_examples or role_config.few_shot_examples\n",
    "            example_text = f\"Example: Q: {examples[0]['query']}\\nA: {examples[0]['response']}\" if examples else \"\"\n",
    "            \n",
    "            return f\"\"\"{base_instruction}\n",
    "\n",
    "{example_text}\n",
    "\n",
    "Now I'll analyze your question step-by-step:\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "REASONING:\n",
    "{role_config.thinking_style}\n",
    "Let me break this down:\n",
    "1. What the question is asking\n",
    "2. Key information from the context\n",
    "3. How they connect\n",
    "\n",
    "COMPLETE ANSWER:\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_confidence(response: str) -> float:\n",
    "        \"\"\"Extract confidence score from response if present.\"\"\"\n",
    "        # Simple heuristic based on response characteristics\n",
    "        confidence = 0.7  # Base confidence\n",
    "        \n",
    "        # Increase confidence for detailed responses\n",
    "        if len(response) > 200:\n",
    "            confidence += 0.1\n",
    "        \n",
    "        # Check for uncertainty markers\n",
    "        uncertainty_phrases = [\"might\", \"possibly\", \"perhaps\", \"unclear\", \"not certain\"]\n",
    "        if any(phrase in response.lower() for phrase in uncertainty_phrases):\n",
    "            confidence -= 0.2\n",
    "        \n",
    "        # Check for confidence markers\n",
    "        confidence_phrases = [\"clearly\", \"definitely\", \"certainly\", \"obviously\"]\n",
    "        if any(phrase in response.lower() for phrase in confidence_phrases):\n",
    "            confidence += 0.1\n",
    "        \n",
    "        return max(0.1, min(1.0, confidence))  # Clamp between 0.1 and 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "self_consistency"
   },
   "source": [
    "### üîÑ Part 7: Self-Consistency and Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "self_cons_code"
   },
   "outputs": [],
   "source": [
    "def generate_with_self_consistency(prompt: str, generator_pipeline, num_samples: int = 3) -> Tuple[str, float, List[str]]:\n",
    "    \"\"\"Generate multiple responses and select the best one.\"\"\"\n",
    "    \n",
    "    if num_samples == 1:\n",
    "        # Standard single generation\n",
    "        outputs = generator_pipeline(prompt)\n",
    "        response = outputs[0]['generated_text'].split(\"ANSWER:\")[-1].strip()\n",
    "        confidence = PromptBuilder.extract_confidence(response)\n",
    "        return response, confidence, [response]\n",
    "    \n",
    "    # Generate multiple responses\n",
    "    responses = []\n",
    "    for i in range(num_samples):\n",
    "        outputs = generator_pipeline(prompt, temperature=0.7 + i*0.1)  # Vary temperature\n",
    "        response = outputs[0]['generated_text'].split(\"ANSWER:\")[-1].strip()\n",
    "        if \"COMPLETE ANSWER:\" in response:\n",
    "            response = response.split(\"COMPLETE ANSWER:\")[-1].strip()\n",
    "        responses.append(response)\n",
    "    \n",
    "    # Simple voting mechanism - find common themes\n",
    "    # For a more sophisticated approach, you could use semantic similarity\n",
    "    word_freq = Counter()\n",
    "    for response in responses:\n",
    "        words = response.lower().split()\n",
    "        word_freq.update(words)\n",
    "    \n",
    "    # Select the response that contains the most common themes\n",
    "    best_response = responses[0]\n",
    "    best_score = 0\n",
    "    \n",
    "    for response in responses:\n",
    "        score = sum(word_freq[word.lower()] for word in response.split())\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_response = response\n",
    "    \n",
    "    # Calculate confidence based on consistency\n",
    "    avg_length = np.mean([len(r) for r in responses])\n",
    "    length_variance = np.var([len(r) for r in responses])\n",
    "    base_confidence = PromptBuilder.extract_confidence(best_response)\n",
    "    \n",
    "    # Lower confidence if responses vary significantly\n",
    "    if length_variance > 10000:  # High variance in response lengths\n",
    "        base_confidence *= 0.8\n",
    "    \n",
    "    return best_response, base_confidence, responses\n",
    "\n",
    "\n",
    "def generate_response(query: str, context: str, task: str, role: str, \n",
    "                             strategy: str, custom_examples: Optional[List[Dict]] = None,\n",
    "                             use_self_consistency: bool = False) -> Dict:\n",
    "    \"\"\"Generate response with selected strategies.\"\"\"\n",
    "    \n",
    "    if text_generator is None:\n",
    "        return {\n",
    "            \"response\": \"Error: Text generator not initialized.\",\n",
    "            \"confidence\": 0.0,\n",
    "            \"prompt_used\": \"\",\n",
    "            \"all_responses\": []\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Build prompt based on task and strategy\n",
    "        if task == \"Ask a question\":\n",
    "            prompt = PromptBuilder.build_qa_prompt(role, query, context, strategy, custom_examples)\n",
    "        else:\n",
    "            # For now, focusing on QA. Can extend to other tasks\n",
    "            prompt = PromptBuilder.build_qa_prompt(role, query, context, strategy, custom_examples)\n",
    "        \n",
    "        # Generate response(s)\n",
    "        num_samples = 3 if use_self_consistency else 1\n",
    "        response, confidence, all_responses = generate_with_self_consistency(\n",
    "            prompt, text_generator, num_samples\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"response\": response,\n",
    "            \"confidence\": confidence,\n",
    "            \"prompt_used\": prompt,\n",
    "            \"all_responses\": all_responses\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return {\n",
    "            \"response\": f\"Error: {str(e)}\",\n",
    "            \"confidence\": 0.0,\n",
    "            \"prompt_used\": \"\",\n",
    "            \"all_responses\": []\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viz_analysis"
   },
   "source": [
    "### üìä Part 8: Visualization and Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "viz_code"
   },
   "outputs": [],
   "source": [
    "def create_confidence_gauge(confidence: float) -> go.Figure:\n",
    "    \"\"\"Create a confidence gauge visualization.\"\"\"\n",
    "    \n",
    "    fig = go.Figure(go.Indicator(\n",
    "        mode = \"gauge+number+delta\",\n",
    "        value = confidence * 100,\n",
    "        domain = {'x': [0, 1], 'y': [0, 1]},\n",
    "        title = {'text': \"Response Confidence\", 'font': {'size': 20}},\n",
    "        delta = {'reference': 70, 'increasing': {'color': \"green\"}},\n",
    "        gauge = {\n",
    "            'axis': {'range': [None, 100], 'tickwidth': 1, 'tickcolor': \"darkblue\"},\n",
    "            'bar': {'color': \"darkblue\"},\n",
    "            'bgcolor': \"white\",\n",
    "            'borderwidth': 2,\n",
    "            'bordercolor': \"gray\",\n",
    "            'steps': [\n",
    "                {'range': [0, 50], 'color': 'lightgray'},\n",
    "                {'range': [50, 80], 'color': 'gray'},\n",
    "                {'range': [80, 100], 'color': 'lightgreen'}\n",
    "            ],\n",
    "            'threshold': {\n",
    "                'line': {'color': \"red\", 'width': 4},\n",
    "                'thickness': 0.75,\n",
    "                'value': 90\n",
    "            }\n",
    "        }\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=250,\n",
    "        margin=dict(l=20, r=20, t=40, b=20),\n",
    "        paper_bgcolor=\"white\",\n",
    "        font={'color': \"darkblue\", 'family': \"Arial\"}\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def compare_responses(responses_dict: Dict[str, Dict]) -> str:\n",
    "    \"\"\"Create a comparison of different strategy responses.\"\"\"\n",
    "    \n",
    "    comparison = \"# üìä Strategy Comparison\\n\\n\"\n",
    "    \n",
    "    for strategy, data in responses_dict.items():\n",
    "        comparison += f\"## {strategy.replace('_', ' ').title()}\\n\"\n",
    "        comparison += f\"**Confidence**: {data['confidence']:.2%}\\n\\n\"\n",
    "        comparison += f\"**Response**:\\n> {data['response'][:200]}...\\n\\n\"\n",
    "        comparison += \"---\\n\\n\"\n",
    "    \n",
    "    return comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enhanced_interface"
   },
   "source": [
    "### üéÆ Part 9: Interactive Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "interface_logic"
   },
   "outputs": [],
   "source": [
    "# Document state management\n",
    "document_state = {\n",
    "    \"file_path\": None,\n",
    "    \"vector_store\": None,\n",
    "    \"indexed_chunks\": None\n",
    "}\n",
    "\n",
    "# Example library management\n",
    "custom_examples = {\n",
    "    \"Teacher\": [],\n",
    "    \"Expert Reviewer\": [],\n",
    "    \"Technical Writer\": []\n",
    "}\n",
    "\n",
    "\n",
    "def process_document(file_obj):\n",
    "    \"\"\"Process uploaded document.\"\"\"\n",
    "    global document_state\n",
    "    \n",
    "    if file_obj is None:\n",
    "        return \"Please upload a PDF document.\", \"\"\n",
    "    \n",
    "    current_file_path = file_obj.name\n",
    "    \n",
    "    if current_file_path != document_state.get(\"file_path\"):\n",
    "        chunks = load_and_chunk_pdf(current_file_path)\n",
    "        if chunks is None:\n",
    "            return \"Error: Failed to load PDF.\", \"\"\n",
    "        \n",
    "        vector_store, indexed_chunks = build_vector_store(chunks, embedder)\n",
    "        if vector_store is None:\n",
    "            return \"Error: Failed to build vector store.\", \"\"\n",
    "        \n",
    "        document_state[\"file_path\"] = current_file_path\n",
    "        document_state[\"vector_store\"] = vector_store\n",
    "        document_state[\"indexed_chunks\"] = indexed_chunks\n",
    "        \n",
    "        preview = f\"Document processed successfully!\\n\\n\"\n",
    "        preview += f\"üìÑ File: {os.path.basename(current_file_path)}\\n\"\n",
    "        preview += f\"üìä Chunks: {len(chunks)}\\n\"\n",
    "        preview += f\"üìù Preview: {chunks[0][:200]}...\"\n",
    "        \n",
    "        return \"Document ready for analysis!\", preview\n",
    "    \n",
    "    return \"Using previously processed document.\", \"Document already loaded.\"\n",
    "\n",
    "\n",
    "def add_custom_example(role, query, response):\n",
    "    \"\"\"Add a custom example to the library.\"\"\"\n",
    "    if not query or not response:\n",
    "        return \"Please provide both query and response.\"\n",
    "    \n",
    "    custom_examples[role].append({\n",
    "        \"query\": query,\n",
    "        \"response\": response\n",
    "    })\n",
    "    \n",
    "    return f\"Example added to {role}'s library! Total examples: {len(custom_examples[role])}\"\n",
    "\n",
    "\n",
    "def preview_prompt(role, query, strategy, use_examples):\n",
    "    \"\"\"Preview the prompt that will be used.\"\"\"\n",
    "    if not query:\n",
    "        return \"Please enter a query to preview the prompt.\"\n",
    "    \n",
    "    # Use dummy context for preview\n",
    "    dummy_context = \"[Document context will be inserted here based on your query]\"\n",
    "    \n",
    "    examples = custom_examples.get(role, []) if use_examples else None\n",
    "    prompt = PromptBuilder.build_qa_prompt(role, query, dummy_context, strategy, examples)\n",
    "    \n",
    "    return f\"```\\n{prompt}\\n```\"\n",
    "\n",
    "\n",
    "def run_analysis(file_obj, role, query, strategy, use_self_consistency, \n",
    "                use_custom_examples, compare_strategies):\n",
    "    \"\"\"Main analysis function.\"\"\"\n",
    "    \n",
    "    # Check models\n",
    "    if embedder is None or text_generator is None:\n",
    "        return \"Error: Models not loaded.\", \"\", None\n",
    "    \n",
    "    # Check document\n",
    "    if document_state[\"vector_store\"] is None:\n",
    "        return \"Please upload and process a document first.\", \"\", None\n",
    "    \n",
    "    if not query:\n",
    "        return \"Please enter a query.\", \"\", None\n",
    "    \n",
    "    # Retrieve context\n",
    "    context = retrieve_context(\n",
    "        query, \n",
    "        document_state[\"vector_store\"], \n",
    "        embedder, \n",
    "        document_state[\"indexed_chunks\"], \n",
    "        top_k=6\n",
    "    )\n",
    "    \n",
    "    if \"Error\" in context:\n",
    "        return f\"Error retrieving context: {context}\", \"\", None\n",
    "    \n",
    "    # Generate response(s)\n",
    "    if compare_strategies:\n",
    "        # Compare all strategies\n",
    "        responses_dict = {}\n",
    "        \n",
    "        for strat in STRATEGIES.keys():\n",
    "            result = generate_response(\n",
    "                query, context, \"Ask a question\", role, strat,\n",
    "                custom_examples[role] if use_custom_examples else None,\n",
    "                use_self_consistency\n",
    "            )\n",
    "            responses_dict[strat] = result\n",
    "        \n",
    "        # Use the selected strategy's response as main\n",
    "        main_result = responses_dict[strategy]\n",
    "        comparison = compare_responses(responses_dict)\n",
    "        \n",
    "        return main_result[\"response\"], comparison, create_confidence_gauge(main_result[\"confidence\"])\n",
    "    \n",
    "    else:\n",
    "        # Single strategy\n",
    "        result = generate_response(\n",
    "            query, context, \"Ask a question\", role, strategy,\n",
    "            custom_examples[role] if use_custom_examples else None,\n",
    "            use_self_consistency\n",
    "        )\n",
    "        \n",
    "        details = f\"**Strategy**: {strategy}\\n\"\n",
    "        details += f\"**Confidence**: {result['confidence']:.2%}\\n\"\n",
    "        if use_self_consistency and len(result['all_responses']) > 1:\n",
    "            details += f\"\\n**Self-Consistency**: Generated {len(result['all_responses'])} responses\\n\"\n",
    "        \n",
    "        return result[\"response\"], details, create_confidence_gauge(result[\"confidence\"])\n",
    "\n",
    "\n",
    "def edit_and_regenerate(edited_prompt):\n",
    "    \"\"\"Generate response from edited prompt.\"\"\"\n",
    "    if not edited_prompt:\n",
    "        return \"Please provide a prompt.\", None\n",
    "    \n",
    "    try:\n",
    "        outputs = text_generator(edited_prompt)\n",
    "        response = outputs[0]['generated_text'].split(edited_prompt)[-1].strip()\n",
    "        \n",
    "        # Clean up common markers\n",
    "        for marker in [\"ANSWER:\", \"RESPONSE:\", \"COMPLETE ANSWER:\"]:\n",
    "            if marker in response:\n",
    "                response = response.split(marker)[-1].strip()\n",
    "        \n",
    "        confidence = PromptBuilder.extract_confidence(response)\n",
    "        return response, create_confidence_gauge(confidence)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "launch_interface"
   },
   "source": [
    "### üöÄ Part 10: Launch Ultra-Smart-AI Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "launch_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Ultra-Smart AI Interface with Multi-Strategy Support...\n",
      "Launching Ultra-Smart AI Interface...\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF: C:\\Users\\sawsa\\AppData\\Local\\Temp\\gradio\\4eaabb6a6bcfa48a3c3920a2173820a3870345000c430f02999e08c5b95c3be3\\AI Agent Course - Complete Structure.pdf\n",
      "Document loaded and split into 70 chunks.\n",
      "Generating embeddings for 70 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3beff292a414c24bfc463c2af527bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created with 70 vectors.\n",
      "Loading PDF: C:\\Users\\sawsa\\AppData\\Local\\Temp\\gradio\\5e21c644ca3f09f42e7385d82d8f2129b05c7c115bb7481870920b1ea572eb19\\prehabilitaatio.pdf\n",
      "Document loaded and split into 53 chunks.\n",
      "Generating embeddings for 53 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4330bb10735944bbaf527fbb31ac1950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created with 53 vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF: C:\\Users\\sawsa\\AppData\\Local\\Temp\\gradio\\5e82911a21ace0179f23a1afffd510ca2af67e337c904ada387a821743baf1ee\\Newwhitepaper_Agents.pdf\n",
      "Document loaded and split into 73 chunks.\n",
      "Generating embeddings for 73 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d748007ae5e44eef8ec10018d35c7991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created with 73 vectors.\n"
     ]
    }
   ],
   "source": [
    "# Create the Smart Gradio interface\n",
    "print(\"Building Ultra-Smart AI Interface with Multi-Strategy Support...\")\n",
    "\n",
    "# Custom CSS\n",
    "custom_css = \"\"\"\n",
    ".strategy-box {\n",
    "    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "    padding: 20px;\n",
    "    border-radius: 10px;\n",
    "    color: white;\n",
    "}\n",
    ".example-box {\n",
    "    background-color: #f7fafc;\n",
    "    padding: 15px;\n",
    "    border-radius: 8px;\n",
    "    border: 1px solid #e2e8f0;\n",
    "}\n",
    ".confidence-indicator {\n",
    "    font-size: 24px;\n",
    "    font-weight: bold;\n",
    "    text-align: center;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks(css=custom_css, title=\"Ultra-Smart AI Assistant\", theme=gr.themes.Soft()) as demo:\n",
    "    \n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # üöÄ Ultra-Smart AI Document Helper\n",
    "        ### Combining Multiple Advanced Prompting Strategies\n",
    "        \n",
    "        **Features**: Role-Based Prompting + Few-Shot Learning + Chain-of-Thought + Self-Consistency + Interactive Editing\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        # Main Analysis Tab\n",
    "        with gr.Tab(\"üìä Document Analysis\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    file_input = gr.File(label=\"üìÑ Upload PDF\", file_types=[\".pdf\"])\n",
    "                    process_btn = gr.Button(\"Process Document\", variant=\"primary\")\n",
    "                    doc_status = gr.Textbox(label=\"Status\", lines=2)\n",
    "                    doc_preview = gr.Textbox(label=\"Document Preview\", lines=4)\n",
    "                    \n",
    "                    gr.Markdown(\"### üéØ Configuration\")\n",
    "                    role_select = gr.Dropdown(\n",
    "                        label=\"üé≠ AI Role\",\n",
    "                        choices=list(ROLES.keys()),\n",
    "                        value=\"Teacher\"\n",
    "                    )\n",
    "                    \n",
    "                    strategy_select = gr.Radio(\n",
    "                        label=\"üìö Prompting Strategy\",\n",
    "                        choices=list(STRATEGIES.keys()),\n",
    "                        value=\"combined\",\n",
    "                        info=\"Select the prompting approach\"\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        use_self_consistency = gr.Checkbox(\n",
    "                            label=\"üîÑ Self-Consistency\",\n",
    "                            value=False,\n",
    "                            info=\"Generate multiple responses\"\n",
    "                        )\n",
    "                        use_custom_examples = gr.Checkbox(\n",
    "                            label=\"üìù Use Custom Examples\",\n",
    "                            value=False\n",
    "                        )\n",
    "                        compare_strategies = gr.Checkbox(\n",
    "                            label=\"üîç Compare All Strategies\",\n",
    "                            value=False\n",
    "                        )\n",
    "                \n",
    "                with gr.Column(scale=2):\n",
    "                    query_input = gr.Textbox(\n",
    "                        label=\"‚ùì Your Question\",\n",
    "                        placeholder=\"What would you like to know about the document?\",\n",
    "                        lines=3\n",
    "                    )\n",
    "                    \n",
    "                    analyze_btn = gr.Button(\"üöÄ Analyze\", variant=\"primary\", size=\"lg\")\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        with gr.Column(scale=2):\n",
    "                            main_response = gr.Textbox(\n",
    "                                label=\"üí¨ AI Response\",\n",
    "                                lines=15\n",
    "                            )\n",
    "                            strategy_details = gr.Markdown(label=\"üìä Details\")\n",
    "                        \n",
    "                        with gr.Column(scale=1):\n",
    "                            confidence_plot = gr.Plot(label=\"Confidence\")\n",
    "        \n",
    "        # Prompt Engineering Tab\n",
    "        with gr.Tab(\"üîß Prompt Engineering\"):\n",
    "            gr.Markdown(\"### üëÅÔ∏è Prompt Preview & Editing\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                preview_role = gr.Dropdown(\n",
    "                    label=\"Role\",\n",
    "                    choices=list(ROLES.keys()),\n",
    "                    value=\"Teacher\"\n",
    "                )\n",
    "                preview_strategy = gr.Dropdown(\n",
    "                    label=\"Strategy\",\n",
    "                    choices=list(STRATEGIES.keys()),\n",
    "                    value=\"combined\"\n",
    "                )\n",
    "                preview_use_examples = gr.Checkbox(label=\"Include Examples\", value=True)\n",
    "            \n",
    "            preview_query = gr.Textbox(\n",
    "                label=\"Query\",\n",
    "                placeholder=\"Enter a query to preview the prompt\"\n",
    "            )\n",
    "            \n",
    "            preview_btn = gr.Button(\"Preview Prompt\")\n",
    "            prompt_preview = gr.Markdown(label=\"Prompt Preview\")\n",
    "            \n",
    "            gr.Markdown(\"### ‚úèÔ∏è Custom Prompt Editor\")\n",
    "            edited_prompt = gr.Textbox(\n",
    "                label=\"Edit Prompt\",\n",
    "                lines=10,\n",
    "                placeholder=\"Paste or write your custom prompt here...\"\n",
    "            )\n",
    "            \n",
    "            regenerate_btn = gr.Button(\"Generate from Custom Prompt\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                custom_response = gr.Textbox(label=\"Response\", lines=10)\n",
    "                custom_confidence = gr.Plot(label=\"Confidence\")\n",
    "        \n",
    "        # Example Library Tab\n",
    "        with gr.Tab(\"üìö Example Library\"):\n",
    "            gr.Markdown(\"### üìù Add Custom Examples for Few-Shot Learning\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                example_role = gr.Dropdown(\n",
    "                    label=\"Role\",\n",
    "                    choices=list(ROLES.keys()),\n",
    "                    value=\"Teacher\"\n",
    "                )\n",
    "            \n",
    "            example_query = gr.Textbox(\n",
    "                label=\"Example Query\",\n",
    "                placeholder=\"What is quantum computing?\"\n",
    "            )\n",
    "            \n",
    "            example_response = gr.Textbox(\n",
    "                label=\"Example Response\",\n",
    "                lines=5,\n",
    "                placeholder=\"Quantum computing is like having a magical coin...\"\n",
    "            )\n",
    "            \n",
    "            add_example_btn = gr.Button(\"Add Example\")\n",
    "            example_status = gr.Textbox(label=\"Status\")\n",
    "            \n",
    "            gr.Markdown(\"### üìñ Current Examples\")\n",
    "            for role_name in ROLES.keys():\n",
    "                with gr.Accordion(f\"{ROLES[role_name].emoji} {role_name} Examples\", open=False):\n",
    "                    examples_text = \"\\n\\n\".join([\n",
    "                        f\"**Q**: {ex['query']}\\n**A**: {ex['response']}\"\n",
    "                        for ex in ROLES[role_name].few_shot_examples\n",
    "                    ])\n",
    "                    gr.Markdown(examples_text or \"No examples yet.\")\n",
    "    \n",
    "    # Event handlers\n",
    "    process_btn.click(\n",
    "        fn=process_document,\n",
    "        inputs=[file_input],\n",
    "        outputs=[doc_status, doc_preview]\n",
    "    )\n",
    "    \n",
    "    analyze_btn.click(\n",
    "        fn=run_analysis,\n",
    "        inputs=[\n",
    "            file_input, role_select, query_input, strategy_select,\n",
    "            use_self_consistency, use_custom_examples, compare_strategies\n",
    "        ],\n",
    "        outputs=[main_response, strategy_details, confidence_plot]\n",
    "    )\n",
    "    \n",
    "    preview_btn.click(\n",
    "        fn=preview_prompt,\n",
    "        inputs=[preview_role, preview_query, preview_strategy, preview_use_examples],\n",
    "        outputs=[prompt_preview]\n",
    "    )\n",
    "    \n",
    "    regenerate_btn.click(\n",
    "        fn=edit_and_regenerate,\n",
    "        inputs=[edited_prompt],\n",
    "        outputs=[custom_response, custom_confidence]\n",
    "    )\n",
    "    \n",
    "    add_example_btn.click(\n",
    "        fn=add_custom_example,\n",
    "        inputs=[example_role, example_query, example_response],\n",
    "        outputs=[example_status]\n",
    "    )\n",
    "    \n",
    "    # Footer\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        ---\n",
    "        ### üõ†Ô∏è Technical Details\n",
    "        - **Strategies**: Standard, Few-Shot, Chain-of-Thought, Combined\n",
    "        - **Models**: {emb} (embeddings), {llm} (generation)\n",
    "        - **Improvement**: Multi-strategy prompting with interactive features\n",
    "        \"\"\".format(emb=EMBEDDING_MODEL_NAME, llm=LLM_MODEL_NAME)\n",
    "    )\n",
    "\n",
    "print(\"Launching Ultra-Smart AI Interface...\")\n",
    "demo.launch(debug=False, share=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
