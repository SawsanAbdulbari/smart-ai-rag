{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### üß™ Final Project: Smart AI - Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "Use this code as the base for your final project. In this code we build an interactive **Gradio application** that combines document understanding with open-source Large Language Models (LLMs) using the **Retrieval-Augmented Generation (RAG)** approach.\n",
        "\n",
        "You will learn how to:\n",
        "\n",
        "- üìÑ Process and chunk PDF documents  \n",
        "- üß† Generate embeddings for semantic search  \n",
        "- üîé Use FAISS for fast vector similarity retrieval  \n",
        "- ü§ñ Prompt LLMs with retrieved context for:\n",
        "  - Question Answering  \n",
        "  - Summarization  \n",
        "  - Translation (to Finnish)\n",
        "\n",
        "This exercise brings together the key components of the RAG pipeline:  \n",
        "**Document Processing ‚Üí Embedding ‚Üí Indexing ‚Üí Retrieval ‚Üí Generation**\n",
        "\n",
        "We‚Äôll use:\n",
        "\n",
        "- üß† **Sentence Transformers** for embeddings  \n",
        "- üîé **FAISS** for similarity search  \n",
        "- ü§ñ **Google Gemma 2B** (or any other open model) for text generation  \n",
        "- üåê **Gradio** for creating a user interface"
      ],
      "metadata": {
        "id": "4_WFbYH_YYOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîß Part 0: Setup and Installations\n",
        "\n",
        "Before we begin, let‚Äôs install the required libraries.\n",
        "\n",
        "We‚Äôll need:\n",
        "- `faiss-cpu` for efficient similarity search and indexing\n",
        "- `gradio` to build our interactive interface\n",
        "- `pypdf` to extract text from PDF files\n",
        "\n"
      ],
      "metadata": {
        "id": "--f_ZlmLYw5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n",
        "!pip install gradio\n",
        "!pip install pypdf"
      ],
      "metadata": {
        "id": "Q6G7oY16S2e5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üì¶ Part 1: Import Required Libraries\n",
        "\n",
        "Now that we‚Äôve installed the necessary packages, let‚Äôs import them into our environment.\n",
        "\n",
        "These include:\n",
        "- Core Python libraries: `os`, `re`, `warnings`\n",
        "- Numerical and ML tools: `torch`, `numpy`, `faiss`\n",
        "- PDF handling: `pypdf`\n",
        "- Embedding model: `sentence-transformers`\n",
        "- Language model pipeline: `transformers`\n",
        "- Interface: `gradio`\n",
        "\n",
        "We‚Äôll also suppress some warnings to keep the output clean.\n"
      ],
      "metadata": {
        "id": "7HiCBSvEZGiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pypdf import PdfReader\n",
        "import warnings\n",
        "\n",
        "# Suppress specific warnings if needed (e.g., from sentence_transformers)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "print(\"Libraries imported successfully.\")"
      ],
      "metadata": {
        "id": "vqaekU9wSrNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Part 2: Configuration ‚Äì Device and Model Selection\n",
        "\n",
        "In this section, we:\n",
        "\n",
        "- Detect whether a GPU is available and set the appropriate device for model execution\n",
        "- Select our embedding and language generation models\n",
        "- Define the target language for translation\n",
        "- Prepare global variables to hold our models (to avoid reloading them on every interaction)"
      ],
      "metadata": {
        "id": "5ZQo9lnoZX10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- Configuration ---\n",
        "# Select device (GPU if available, otherwise CPU)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU. Note: LLM inference will be significantly slower.\")\n",
        "\n",
        "# --- Model Selection ---\n",
        "# Embedding Model (Choose one)\n",
        "# 'all-MiniLM-L6-v2' is fast and efficient.\n",
        "# 'all-mpnet-base-v2' offers higher quality embeddings.\n",
        "EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'\n",
        "\n",
        "# Large Language Model (Choose one)\n",
        "# Smaller models (< 3B params) are recommended for easier running without large GPUs.\n",
        "# Examples: 'google/gemma-2b-it', 'microsoft/phi-2', 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
        "LLM_MODEL_NAME = \"google/gemma-2b-it\" # Using Gemma 2B Instruct as an example\n",
        "\n",
        "# Target language for translation task\n",
        "TARGET_LANGUAGE = \"Finnish\"\n",
        "\n",
        "# --- Global Variables (Loaded Models) ---\n",
        "# We load models globally to avoid reloading them on every Gradio interaction.\n",
        "embedder = None\n",
        "text_generator = None\n",
        "tokenizer = None # Keep tokenizer for potential manual formatting if needed"
      ],
      "metadata": {
        "id": "qJqeYLPPVEbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîë Part 3: Authenticate with Hugging Face Hub\n",
        "\n",
        "To download models from the Hugging Face Hub‚Äîespecially larger or gated models‚Äîyou may need to authenticate with your Hugging Face account.\n",
        "\n",
        "Run the following command and paste in your **Hugging Face token** when prompted\n"
      ],
      "metadata": {
        "id": "RHE1Wvo-Zh91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "d9FgmpSfVzIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ü§ñ Part 4: Load Models ‚Äì Embedder & LLM\n",
        "\n",
        "In this step, we load:\n",
        "\n",
        "- The **Sentence Transformer** model for generating document embeddings.\n",
        "- The **Large Language Model (LLM)** for response generation.\n",
        "\n",
        "We also configure:\n",
        "- **4-bit quantization** (optional) for memory-efficient LLM loading using `bitsandbytes`.\n",
        "- Automatic selection of optimal `torch_dtype` based on hardware (e.g., `bfloat16` or `float16`).\n"
      ],
      "metadata": {
        "id": "RgSiV3buZyIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load Models (Generator LLM & Embedder)\n",
        "\n",
        "def load_models():\n",
        "    \"\"\"Loads the embedding and language models.\"\"\"\n",
        "    global embedder, text_generator, tokenizer\n",
        "\n",
        "    print(f\"Loading embedding model: {EMBEDDING_MODEL_NAME}...\")\n",
        "    try:\n",
        "        embedder = SentenceTransformer(EMBEDDING_MODEL_NAME, device=device)\n",
        "        print(\"Embedding model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading embedding model: {e}\")\n",
        "        raise\n",
        "\n",
        "    print(f\"Loading LLM: {LLM_MODEL_NAME}...\")\n",
        "    try:\n",
        "        # Optional: Configuration for loading in 4-bit for memory savings\n",
        "        # Requires 'bitsandbytes' library\n",
        "        use_4bit = True # Set to False if you don't want 4-bit or have issues\n",
        "        bnb_config = None\n",
        "        if use_4bit and torch.cuda.is_available():\n",
        "            try:\n",
        "                bnb_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_quant_type=\"nf4\",\n",
        "                    bnb_4bit_compute_dtype=torch.bfloat16, # Use bfloat16 for faster computation if supported\n",
        "                    bnb_4bit_use_double_quant=False,\n",
        "                )\n",
        "                print(\"Using 4-bit quantization.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Could not set up 4-bit quantization, proceeding without it: {e}\")\n",
        "                bnb_config = None # Fallback if BitsAndBytesConfig fails\n",
        "        elif use_4bit:\n",
        "            print(\"4-bit quantization requires a CUDA GPU. Proceeding without it.\")\n",
        "\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
        "\n",
        "        # Determine torch_dtype based on device and availability\n",
        "        if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
        "             model_dtype = torch.bfloat16\n",
        "             print(\"Using bfloat16 dtype.\")\n",
        "        else:\n",
        "             model_dtype = torch.float16 # Use float16 as a fallback on GPU or CPU\n",
        "             print(\"Using float16 dtype.\")\n",
        "\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            LLM_MODEL_NAME,\n",
        "            device_map=\"auto\",  # Automatically map model layers to available devices (GPU/CPU/Disk)\n",
        "            torch_dtype=model_dtype,\n",
        "            quantization_config=bnb_config, # Apply 4-bit config if defined\n",
        "            trust_remote_code=True # Needed for some models like Phi-2\n",
        "        )\n",
        "        print(\"LLM loaded successfully.\")\n",
        "\n",
        "        # Create a text generation pipeline\n",
        "        # Note: max_new_tokens limits the length of the *generated* response.\n",
        "        text_generator = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            max_new_tokens=300, # Increased max tokens for potentially longer summaries/translations\n",
        "            # temperature=0.7, # Controls randomness (higher = more random)\n",
        "            # top_p=0.9,       # Nucleus sampling (considers top p% probability mass)\n",
        "            do_sample=True,   # Enable sampling for more creative responses\n",
        "            framework=\"pt\" # Specify PyTorch framework\n",
        "        )\n",
        "        print(\"Text generation pipeline ready.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading LLM or creating pipeline: {e}\")\n",
        "        raise\n",
        "\n",
        "# --- Call the loading function once at the start ---\n",
        "try:\n",
        "    load_models()\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load models. The application might not work correctly. Error: {e}\")\n",
        "    # Optionally exit or handle this case more gracefully depending on deployment context\n",
        "    # exit()\n",
        "\n"
      ],
      "metadata": {
        "id": "3QdeLg10VRHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìÑ Part 5: Document Processing & Vector Store Creation\n",
        "\n",
        "In this step, we:\n",
        "\n",
        "1. **Extract text** from the uploaded PDF\n",
        "2. **Split the text into manageable chunks** using a sliding window approach\n",
        "3. **Generate embeddings** for each chunk using the embedding model\n",
        "4. **Store the embeddings** in a FAISS index for fast similarity-based retrieval\n",
        "\n",
        "This sets the foundation for enabling semantic search over the document content."
      ],
      "metadata": {
        "id": "9gfVsNA_aJeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Document Processing and Vector Store Creation\n",
        "\n",
        "def load_and_chunk_pdf(file_path, chunk_size=700, chunk_overlap=70):\n",
        "    \"\"\"\n",
        "    Loads text from a PDF file, cleans it, and splits it into overlapping chunks.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the PDF file.\n",
        "        chunk_size (int): The approximate size of each text chunk.\n",
        "        chunk_overlap (int): The number of characters to overlap between consecutive chunks.\n",
        "\n",
        "    Returns:\n",
        "        list[str]: A list of text chunks, or None if processing fails.\n",
        "    \"\"\"\n",
        "    if not file_path or not os.path.exists(file_path):\n",
        "        print(f\"Error: PDF file not found at {file_path}\")\n",
        "        return None\n",
        "    try:\n",
        "        print(f\"Loading PDF: {file_path}\")\n",
        "        reader = PdfReader(file_path)\n",
        "        text = \"\"\n",
        "        for i, page in enumerate(reader.pages):\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \"\\n\" # Add newline between pages\n",
        "            else:\n",
        "                print(f\"Warning: Could not extract text from page {i+1}.\")\n",
        "\n",
        "        if not text:\n",
        "            print(\"Error: No text extracted from the PDF.\")\n",
        "            return None\n",
        "\n",
        "        # Basic cleaning\n",
        "        text = re.sub(r'\\n\\s*\\n', '\\n', text).strip() # Remove multiple blank lines\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()      # Replace multiple spaces with single space\n",
        "\n",
        "        # Simple sliding window chunking\n",
        "        chunks = []\n",
        "        start_index = 0\n",
        "        while start_index < len(text):\n",
        "            end_index = start_index + chunk_size\n",
        "            chunks.append(text[start_index:end_index])\n",
        "            start_index += chunk_size - chunk_overlap # Move window\n",
        "\n",
        "        # Filter out very short chunks that might result from the end of the text\n",
        "        chunks = [chunk for chunk in chunks if len(chunk.strip()) > 50]\n",
        "\n",
        "        print(f\"Document loaded and split into {len(chunks)} chunks.\")\n",
        "        return chunks\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or chunking PDF '{file_path}': {e}\")\n",
        "        return None\n",
        "\n",
        "def build_vector_store(chunks, embedder_model):\n",
        "    \"\"\"\n",
        "    Generates embeddings for text chunks and builds a FAISS index for fast retrieval.\n",
        "\n",
        "    Args:\n",
        "        chunks (list[str]): The list of text chunks.\n",
        "        embedder_model: The loaded SentenceTransformer model.\n",
        "\n",
        "    Returns:\n",
        "        tuple(faiss.Index, list[str]): The FAISS index and the original chunks, or (None, None) if failed.\n",
        "    \"\"\"\n",
        "    if not chunks or embedder_model is None:\n",
        "        print(\"Error: No chunks or embedder model provided for vector store creation.\")\n",
        "        return None, None\n",
        "    try:\n",
        "        print(f\"Generating embeddings for {len(chunks)} chunks...\")\n",
        "        # Generate embeddings (move embedder to CPU temporarily if it's on GPU and causes memory issues here)\n",
        "        # This depends on available VRAM vs. embedding model size.\n",
        "        # For smaller models like MiniLM, GPU is usually fine.\n",
        "        embeddings = embedder_model.encode(chunks, convert_to_tensor=False, show_progress_bar=True) # Get numpy arrays directly\n",
        "\n",
        "        # Ensure embeddings are float32 for FAISS\n",
        "        embeddings_np = np.array(embeddings).astype('float32')\n",
        "\n",
        "        # Create FAISS index (using L2 distance for similarity)\n",
        "        embedding_dim = embeddings_np.shape[1]\n",
        "        index = faiss.IndexFlatL2(embedding_dim)\n",
        "        index.add(embeddings_np)\n",
        "\n",
        "        print(f\"FAISS index created successfully with {index.ntotal} vectors.\")\n",
        "        return index, chunks # Return chunks for easy lookup later\n",
        "    except Exception as e:\n",
        "        print(f\"Error building vector store: {e}\")\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "ga8c62D-W-kG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîé Part 6: Semantic Retrieval from Vector Store\n",
        "\n",
        "Once the PDF has been processed and indexed, we can now perform **semantic search** to retrieve the most relevant content based on the user's input.\n",
        "\n",
        "This function retrieves the top `k` chunks from the vector store that are most semantically similar to the query using **FAISS similarity search**.\n"
      ],
      "metadata": {
        "id": "BUZ17bFcaW_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Retrieval\n",
        "\n",
        "def retrieve_context(query, vector_store, embedder_model, indexed_chunks, top_k=3):\n",
        "    \"\"\"\n",
        "    Retrieves the top_k most relevant text chunks from the vector store based on the query.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's query.\n",
        "        vector_store (faiss.Index): The FAISS index.\n",
        "        embedder_model: The loaded SentenceTransformer model.\n",
        "        indexed_chunks (list[str]): The original chunks corresponding to the index vectors.\n",
        "        top_k (int): The number of relevant chunks to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        str: A string containing the concatenated relevant chunks, or an error message.\n",
        "    \"\"\"\n",
        "    if vector_store is None or embedder_model is None or indexed_chunks is None:\n",
        "        return \"Error: Vector store, embedder, or chunks not initialized.\"\n",
        "    try:\n",
        "        print(f\"Retrieving context for query: '{query}'\")\n",
        "        # Generate embedding for the query\n",
        "        query_embedding = embedder_model.encode([query], convert_to_tensor=False) # Get numpy array\n",
        "        query_embedding_np = np.array(query_embedding).astype('float32')\n",
        "\n",
        "        # Search the FAISS index\n",
        "        distances, indices = vector_store.search(query_embedding_np, top_k)\n",
        "\n",
        "        # Get the actual text chunks based on the indices\n",
        "        retrieved_chunks = [indexed_chunks[i] for i in indices[0] if 0 <= i < len(indexed_chunks)]\n",
        "\n",
        "        if not retrieved_chunks:\n",
        "            print(\"Warning: No relevant chunks found for the query.\")\n",
        "            return \"Could not find relevant context for this query in the document.\"\n",
        "\n",
        "        # Combine the chunks into a single context string\n",
        "        context = \"\\n\\n---\\n\\n\".join(retrieved_chunks) # Separate chunks clearly\n",
        "        print(f\"Retrieved {len(retrieved_chunks)} chunks.\")\n",
        "        return context\n",
        "    except Exception as e:\n",
        "        print(f\"Error during context retrieval: {e}\")\n",
        "        return f\"Error retrieving context: {e}\"\n",
        "\n"
      ],
      "metadata": {
        "id": "NkCsDmstXB8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Part 7: RAG Response Generation with Prompt Templates\n",
        "\n",
        "We now define task-specific **prompt templates** that guide the language model to produce accurate, contextual responses. This is a key feature of **Retrieval-Augmented Generation (RAG)**, where retrieved chunks are combined with smart prompting.\n"
      ],
      "metadata": {
        "id": "Z9o6v8o_au9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG Generation (with Task-Specific Prompts)\n",
        "\n",
        "# --- Prompt Templates ---\n",
        "# Define templates for different tasks. These guide the LLM on how to use the context.\n",
        "# Note: Gemma-Instruct uses a specific chat format. We'll format the prompt string\n",
        "#       and rely on the pipeline to handle the underlying model specifics, but\n",
        "#       for optimal results, using the model's chat template directly might be better.\n",
        "\n",
        "QA_PROMPT_TEMPLATE = \"\"\"SYSTEM: Use the following context to answer the question concisely.\n",
        "If the answer is not found in the context, state that you cannot answer based on the provided information. Do not make up information.\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "USER: {query}\n",
        "\n",
        "ASSISTANT:\"\"\"\n",
        "\n",
        "\n",
        "SUMMARIZATION_PROMPT_TEMPLATE = \"\"\"SYSTEM: Based *only* on the following text, provide a concise summary. Focus on the main points.\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "USER: Provide a summary of the context above.\n",
        "\n",
        "ASSISTANT: Summary:\"\"\"\n",
        "\n",
        "# CORRECTED DEFINITION: Use double braces {{context}} to escape it in the f-string.\n",
        "# This leaves {context} as a literal placeholder for the .format() call later.\n",
        "TRANSLATION_PROMPT_TEMPLATE = f\"\"\"SYSTEM: Translate the following text accurately into {TARGET_LANGUAGE}. Provide only the translation.\n",
        "\n",
        "CONTEXT:\n",
        "{{context}}\n",
        "\n",
        "USER: Translate the context above into {TARGET_LANGUAGE}.\n",
        "\n",
        "ASSISTANT: Translation ({TARGET_LANGUAGE}):\"\"\"\n",
        "\n",
        "\n",
        "def generate_response(query, context, task_prompt_template, generator_pipeline):\n",
        "    \"\"\"\n",
        "    Generates a response using the LLM pipeline based on the query, retrieved context,\n",
        "    and a task-specific prompt template.\n",
        "\n",
        "    Args:\n",
        "        query (str): The original user query (used within the prompt template).\n",
        "        context (str): The retrieved context chunks.\n",
        "        task_prompt_template (str): The prompt template for the specific task.\n",
        "        generator_pipeline: The Hugging Face text-generation pipeline.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated response from the LLM, or an error message.\n",
        "    \"\"\"\n",
        "    if generator_pipeline is None:\n",
        "        return \"Error: Text generation pipeline not initialized.\"\n",
        "    # Check if context retrieval itself returned an error or no context\n",
        "    if not context or \"Error:\" in context or \"Could not find relevant context\" in context:\n",
        "         # Pass the specific error message from retrieval\n",
        "         return f\"Cannot generate response because context retrieval failed: {context}\"\n",
        "\n",
        "    try:\n",
        "        # Format the final prompt using the template, context, and query\n",
        "        # The .format() call will correctly substitute the actual 'context' variable\n",
        "        # into the {context} placeholder within the template string.\n",
        "        prompt = task_prompt_template.format(context=context, query=query)\n",
        "\n",
        "        print(\"Generating response with LLM...\")\n",
        "        # print(f\"--- Prompt Start ---\\n{prompt}\\n--- Prompt End ---\") # Uncomment for debugging\n",
        "\n",
        "        # Use the pipeline for generation\n",
        "        outputs = generator_pipeline(prompt)\n",
        "        generated_text = outputs[0]['generated_text']\n",
        "\n",
        "        # Clean up the response: Remove the prompt from the generated text\n",
        "        assistant_marker = \"ASSISTANT:\"\n",
        "        marker_pos = generated_text.rfind(assistant_marker)\n",
        "        if marker_pos != -1:\n",
        "            response = generated_text[marker_pos + len(assistant_marker):].strip()\n",
        "            # Further cleanup for specific tasks if needed\n",
        "            if task_prompt_template == SUMMARIZATION_PROMPT_TEMPLATE and response.startswith(\"Summary:\"):\n",
        "                 response = response[len(\"Summary:\"):].strip()\n",
        "            # Check against the dynamically formatted TARGET_LANGUAGE string\n",
        "            translation_marker = f\"Translation ({TARGET_LANGUAGE}):\"\n",
        "            if task_prompt_template == TRANSLATION_PROMPT_TEMPLATE and response.startswith(translation_marker):\n",
        "                 response = response[len(translation_marker):].strip()\n",
        "\n",
        "        else:\n",
        "             # Fallback cleanup if ASSISTANT marker isn't found\n",
        "             response = generated_text.replace(prompt, \"\").strip()\n",
        "\n",
        "\n",
        "        print(\"LLM Generation complete.\")\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during LLM generation: {e}\")\n",
        "        if \"CUDA out of memory\" in str(e):\n",
        "             return \"Error: GPU out of memory during generation. Try a smaller model, shorter document, or enable 4-bit quantization if not already active.\"\n",
        "        return f\"Error generating response: {e}\"\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "rYATX2YCSY7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Part 7: Gradio Interface Logic ‚Äì Backend Functionality\n",
        "\n",
        "This section handles all the logic that powers our Gradio interface.\n",
        "\n",
        "When a user uploads a PDF and selects a task, this function:\n",
        "1. Processes the uploaded document if it hasn't been processed already\n",
        "2. Builds or retrieves the document‚Äôs vector store\n",
        "3. Uses semantic search to retrieve relevant content\n",
        "4. Generates a task-specific response using a prompt and the language model"
      ],
      "metadata": {
        "id": "tabx-z0sbg6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Gradio Interface Logic\n",
        "\n",
        "# --- Store vector store state IN MEMORY (Simple approach for demo) ---\n",
        "document_state = {\n",
        "    \"file_path\": None,\n",
        "    \"vector_store\": None,\n",
        "    \"indexed_chunks\": None\n",
        "}\n",
        "\n",
        "def process_document_and_query(file_obj, task, query):\n",
        "    \"\"\"\n",
        "    Main function called by the Gradio interface.\n",
        "    Handles PDF processing, vector store creation/update, context retrieval,\n",
        "    and response generation based on the selected task.\n",
        "    \"\"\"\n",
        "    global document_state # Use the global state\n",
        "\n",
        "    status_message = \"\"\n",
        "    result_output = \"\"\n",
        "\n",
        "    # --- Step 1: Check Models ---\n",
        "    if embedder is None or text_generator is None:\n",
        "        status_message = \"Error: Models not loaded. Please check console.\"\n",
        "        print(status_message)\n",
        "        return status_message, result_output\n",
        "\n",
        "    # --- Step 2: Process Document (if new or not processed) ---\n",
        "    current_file_path = file_obj.name if file_obj else None\n",
        "\n",
        "    if current_file_path is None:\n",
        "        status_message = \"Please upload a PDF document.\"\n",
        "        return status_message, result_output\n",
        "\n",
        "    if current_file_path != document_state.get(\"file_path\"):\n",
        "        status_message = f\"Processing new document: {os.path.basename(current_file_path)}...\"\n",
        "        print(status_message)\n",
        "        document_state = {\"file_path\": None, \"vector_store\": None, \"indexed_chunks\": None} # Reset state\n",
        "\n",
        "        chunks = load_and_chunk_pdf(current_file_path)\n",
        "        if chunks is None:\n",
        "            status_message = \"Error: Failed to load or chunk the PDF.\"\n",
        "            print(status_message)\n",
        "            return status_message, result_output\n",
        "\n",
        "        vector_store, indexed_chunks = build_vector_store(chunks, embedder)\n",
        "        if vector_store is None:\n",
        "            status_message = \"Error: Failed to build the vector store.\"\n",
        "            print(status_message)\n",
        "            return status_message, result_output\n",
        "\n",
        "        document_state[\"file_path\"] = current_file_path\n",
        "        document_state[\"vector_store\"] = vector_store\n",
        "        document_state[\"indexed_chunks\"] = indexed_chunks\n",
        "        status_message = \"Document processed successfully. Ready for tasks.\"\n",
        "        print(status_message)\n",
        "    else:\n",
        "        status_message = f\"Using previously processed document: {os.path.basename(current_file_path)}\"\n",
        "        print(status_message)\n",
        "        vector_store = document_state[\"vector_store\"]\n",
        "        indexed_chunks = document_state[\"indexed_chunks\"]\n",
        "        if vector_store is None or indexed_chunks is None:\n",
        "             status_message = \"Error: Document state is invalid. Please re-upload.\"\n",
        "             print(status_message)\n",
        "             document_state = {\"file_path\": None, \"vector_store\": None, \"indexed_chunks\": None} # Reset\n",
        "             return status_message, result_output\n",
        "\n",
        "\n",
        "    # --- Step 3: Perform Selected Task ---\n",
        "    print(f\"Task selected: {task}\")\n",
        "\n",
        "    # Determine prompt template, context query, and top_k based on task\n",
        "    if task == \"Ask a question\":\n",
        "        if not query:\n",
        "            status_message += \"\\nPlease enter a question.\"\n",
        "            return status_message, result_output\n",
        "        prompt_template = QA_PROMPT_TEMPLATE\n",
        "        context_query = query\n",
        "        top_k = 3\n",
        "    elif task == \"Summarize\":\n",
        "        prompt_template = SUMMARIZATION_PROMPT_TEMPLATE\n",
        "        context_query = \"Provide a comprehensive overview of the document's content.\"\n",
        "        top_k = 6\n",
        "    # Check the task name including the language from the dropdown choices\n",
        "    elif task == f\"Translate (to {TARGET_LANGUAGE})\":\n",
        "        prompt_template = TRANSLATION_PROMPT_TEMPLATE\n",
        "        if not query:\n",
        "            # Ensure indexed_chunks is not empty before accessing index 0\n",
        "            if not indexed_chunks:\n",
        "                 status_message += \"\\nError: Cannot translate without document content.\"\n",
        "                 return status_message, result_output\n",
        "            context_query = indexed_chunks[0][:150] + \"...\"\n",
        "            print(f\"No specific query for translation, using first chunk topic: '{context_query}'\")\n",
        "        else:\n",
        "            context_query = query\n",
        "        top_k = 3\n",
        "    else:\n",
        "        status_message += \"\\nError: Invalid task selected.\"\n",
        "        return status_message, result_output\n",
        "\n",
        "    # Retrieve context\n",
        "    context = retrieve_context(context_query, vector_store, embedder, indexed_chunks, top_k=top_k)\n",
        "\n",
        "    # Generate response (handle potential errors from retrieval)\n",
        "    if \"Error:\" in context or \"Could not find relevant context\" in context:\n",
        "         result_output = f\"Failed to retrieve context: {context}\" # Show retrieval error\n",
        "    else:\n",
        "         result_output = generate_response(query, context, prompt_template, text_generator)\n",
        "\n",
        "    status_message += f\"\\nTask '{task}' completed.\"\n",
        "    print(status_message)\n",
        "\n",
        "    return status_message, result_output\n"
      ],
      "metadata": {
        "id": "TUUzwPTSbZfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üñ•Ô∏è Part 8: Create and Launch the Gradio Interface\n",
        "\n",
        "Now that we have all the logic in place for document processing, retrieval, and generation, let's wrap everything up into a user-friendly **Gradio interface**.\n",
        "\n",
        "This interface allows users to:\n",
        "- Upload a PDF document\n",
        "- Choose a task: Question Answering, Summarization, or Translation\n",
        "- Enter a relevant query or topic\n",
        "- Receive a smart, LLM-generated response based on the document content"
      ],
      "metadata": {
        "id": "ZnmiXQuwbQK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and Launch the Gradio Interface\n",
        "\n",
        "print(\"Setting up Gradio interface...\")\n",
        "\n",
        "# Define Gradio components\n",
        "file_input = gr.File(label=\"Upload PDF Document\", file_types=[\".pdf\"])\n",
        "task_dropdown = gr.Dropdown(\n",
        "    label=\"Select Task\",\n",
        "    # Ensure the choices list matches the checks in process_document_and_query\n",
        "    choices=[\"Ask a question\", \"Summarize\", f\"Translate (to {TARGET_LANGUAGE})\"],\n",
        "    value=\"Ask a question\"\n",
        ")\n",
        "query_input = gr.Textbox(\n",
        "    label=\"Enter Question or Topic\",\n",
        "    info=\"Required for 'Ask a question'. Optional for 'Translate' (specifies topic) or 'Summarize' (ignored).\"\n",
        ")\n",
        "status_output = gr.Textbox(label=\"Status\", interactive=False)\n",
        "result_output = gr.Textbox(label=\"Result\", lines=15, interactive=False)\n",
        "\n",
        "# Create the interface\n",
        "iface = gr.Interface(\n",
        "    fn=process_document_and_query,\n",
        "    inputs=[\n",
        "        file_input,\n",
        "        task_dropdown,\n",
        "        query_input\n",
        "    ],\n",
        "    outputs=[\n",
        "        status_output,\n",
        "        result_output\n",
        "    ],\n",
        "    title=\"Smart Document Helper (RAG + Open Models)\",\n",
        "    description=f\"Upload a PDF, select a task (Q&A, Summarize, Translate to {TARGET_LANGUAGE}), and enter a query if needed.\\n\"\n",
        "                f\"Uses '{EMBEDDING_MODEL_NAME}' for retrieval and '{LLM_MODEL_NAME}' for generation.\",\n",
        "    allow_flagging='never',\n",
        "    examples=[\n",
        "        [None, \"Ask a question\", \"What is the main purpose of this document?\"],\n",
        "        [None, \"Summarize\", \"\"],\n",
        "        [None, f\"Translate (to {TARGET_LANGUAGE})\", \"Explain the methodology used.\"]\n",
        "    ],\n",
        "    cache_examples=False\n",
        ")\n",
        "\n",
        "print(\"Launching Gradio interface...\")\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch(debug=False, share=True)\n",
        "\n",
        "print(\"Gradio setup complete. Interface should be running.\")\n"
      ],
      "metadata": {
        "id": "lSStb-I9bFv5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}